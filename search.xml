<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark计算引擎]]></title>
    <url>%2Fblogs%2F2018%2F03%2F05%2FSpark%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[一、Spark简介 由加州大学伯克利分校的AMP实验室开源 大规模分布式通用计算引擎 具有高吞吐、低延时、通用易扩展、高容错等特点 使用Scala语言开发，提供了丰富的开发API，支持Scala、Java、 Python、R开发语言 Spark提供多种运行模式[ Spark架构](https://s1.ax2x.com/2018/03/05/rpgEO.png) 二、Spark特点 计算高效 使用内存计算引擎，提供Cache缓存机制支持迭代计算或多次数据共享，减少数据读取的IO开销 DAG引擎，减少多次计算之间中间结果写到HDFS的开销 使用多线程池模型来减少task启动开销，shuffle过程中避免不必要的 sort操作以及减少磁盘IO操作 通用易用 提供了丰富的开发API，支持Scala、Java、Python、R开发语言 集成批处理、流处理、交互式计算、机器学习算法、图计算 运行模式多样 Local、Standalone、Yarn、Mesos[ 运行比对图](https://s1.ax2x.com/2018/03/05/rpqVe.png) 三、Spark核心概念-RDD RDD：Resilient Distributed Datasets弹性分布式数据集 Spark基于RDD进行计算 分布在集群中的只读对象集合（由多个Partition构成） 可以存储在磁盘或内存中 可以通过并行转换操作构造 失效后自动重构[ RDD](https://s1.ax2x.com/2018/03/05/rs9Fr.png) 1.RDD操作 Transformation 将Scala集合或者Hadoop数据集构造一个新的RDD 通过已有的RDD产生新的RDD 只记录转换关系，不触发计算 如：map、filter等-Action 通过RDD计算得到一个或者一组值 真正触发执行 如：count、collect、saveAsTextFile [ RDD操作](https://s1.ax2x.com/2018/03/05/rsBAi.png) 2.RDD操作示例1rdd1.map(_+1).saveAsTextFile(&quot;hdfs://node01:9000/&quot;) [ RDD输出](https://s1.ax2x.com/2018/03/05/rsFwy.png) 3.Transformation与Action对比 接口定义方式不同 Transformation：RDD[X] -&gt; RDD[Y] Action：RDD[X] -&gt; Z 执行计算方式不同 Transformation采用惰性执行方式，只记录RDD转化关系，不会触发真正计算执行 Action真正触发计算执行4.Transformation Lazy Execution 惰性执行 [ 懒惰执行](https://s1.ax2x.com/2018/03/05/rsXEl.png) 5.程序执行流程 Spark中的WordCount 12345val rdd1 = sc.textFile(&quot;hdfs://192.168.183.101:9000/data/wc/in&quot;)val rdd2 = rdd1.flatMap(_.split(&quot;\t&quot;))val rdd3= rdd2.map((_,1))val rdd4 = rdd3.reduceByKey(_ + _)rdd4.saveAsTextFile(“hdfs://192.168.183.100:9000/data/wc/out”) [ Spark程序执行流程](https://s1.ax2x.com/2018/03/05/rsYy6.png) 6.RDD Dependency依赖 Narrow Dependency窄依赖 父RDD中的分区最多只能被一个子RDD的一个分区使用 子RDD如果只有部分分区数据丢失 或者损坏只需要从对应的父RDD重新计算恢复 Shuffle Dependency宽依赖 子RDD分区依赖父RDD所有分区 子RDD如果部分分区或者全部分区 数据丢失或者损坏需要从所有父RDD重新计算，相对窄依赖付出的代价更高，尽量避免宽依赖的使用 [ 窄依赖](https://s1.ax2x.com/2018/03/05/rsWH3.png) [ 宽依赖](https://s1.ax2x.com/2018/03/05/rs08p.png) 7.RDD Cache缓存 Spark允许将RDD缓存到内存或磁盘上，方便重用，提高性能 Spark提供了多种缓存级别 用户可以根据实际需求进行调整 123val rdd = sc.textFile(inputArg)rdd.cache()//实际上是调用了rdd.persist(StorageLevel.MEMORY_ONLY)//data.persist(StorageLevel.MEMORY_AND_DISK) [ RDD缓存](https://s1.ax2x.com/2018/03/05/rsp72.png) 四、Spark程序架构 Driver：一个Spark程序有一个Driver，一个Driver创建一个Spark Context，程序的main函数运行在Driver中。Driver主要负责Spark程序的解析、划分Stage，调度Task到Executor上执行 SparkContext：加载配置信息，初始化Spark程序运行环境，创建内部的DAGScheduler和TaskScheduler Executor：负责执行Driver分发的Task任务，集群中一个节点可以启动多个Executor，每个一个Executor通过多线程运行多个Task任务 Task：Spark运行的基本单位，一个Task负责处理RDD一个分区的计算逻辑[ Spark程序架构](https://s1.ax2x.com/2018/03/05/rsOGE.png) 五、Spark运行模式 Local本地模式 单机运行，通常用于测试 Standalone独立模式 Spark集群单独运行 Yarn/Mesos 运行在其他资源管理系统上，如Yarn、Mesos1.Spark Local模式 Local本地模式 将Spark应用以多线程方式，直接运行在本地，方便调试 本地模式分类123local：只启动一个线程运行executorlocal[n]：启动n个线程运行executorlocal[*]：启动跟cpu数目相同的executor 2.Spark Standalone模式 集群独立部署[ Spark standalone模式](https://s1.ax2x.com/2018/03/05/rsuYa.png) 3.YARN程序运行流程[ Yarn执行流程](https://s1.ax2x.com/2018/03/05/rsJ8S.png) 4.Spark YARN模式 yarn-cluster [ Spark on yarn](https://s1.ax2x.com/2018/03/05/rshHh.png) 5.Spark内部执行流程[ Spark 内部执行流程](https://s1.ax2x.com/2018/03/05/rsv4H.png) 生成逻辑查询计划12345sc.textFile(inputArg).flatMap(_.split(&quot;\t&quot;)).map((_,1)).reduceByKey(_ + _).saveAsTextFile(outArg) [ 逻辑查询计划](https://s1.ax2x.com/2018/03/05/rusOi.png) 生成物理查询计划[ 物理查询计划](https://s1.ax2x.com/2018/03/05/ru4nX.png) 任务调度与执行[ 任务调度与执行](https://s1.ax2x.com/2018/03/05/ruhcJ.png) 六、Spark调度模块 DAG：Directed Acyclic Graph有向无环图 DAGScheduler 根据计算任务的依赖关系建立DAG 根据依赖关系是否是宽依赖，将DAG划分为不同的Stage阶段 将各个Stage中的Task组成的TaskSet提交到TaskScheduler TaskScheduler 负责Application的Job调度 重新提交执行失败的Task 为执行速度慢的Task启动备份任务[ Sparkr 模块调度](https://s1.ax2x.com/2018/03/05/ruPX6.png) 1.Spark任务类型和Job划分 Spark中task类型 ShuffleMapTask：除了最后一个输出Task，其他Task类型都是 ShuffleMapTask ResultTask：只有最后一个阶段输出的Task是ResultTask Appication中调用一次Action就会生成一个Job[ Spark 任务划分](https://s1.ax2x.com/2018/03/05/rucN3.png)]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Spark Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume构建日志采集系统]]></title>
    <url>%2Fblogs%2F2018%2F02%2F03%2FFlume%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[一、Flume介绍1.Flume特点 Flume是一个分布式的、可靠的、高可用的海量日志采集、聚合和传输的系统 数据流模型：Source-Channel-Sink 事务机制保证消息传递的可靠性 内置丰富插件，轻松与其他系统集成 Java实现，优秀的系统框架设计，模块分明，易于开发 2.Flume原型图 Flume原型图.png 3.Flume基本组件 Event：消息的基本单位，有header和body组成 Agent：JVM进程，负责将一端外部来源产生的消息转 发到另一端外部的目的地 Source：从外部来源读入event，并写入channel Channel：event暂存组件，source写入后，event将会 一直保存, Sink：从channel读入event，并写入目的地3.Flume事件流 Flume事件流.png 4.Flumes数据流 Flume数据流.png Flume数据流2.png 二、Flume搭建1.下载二进制安装包下载地址：http://flume.apache.org/download.html2.安装Flume解压缩安装包文件 1234567891011121314151617[hadoop@hadoop01 apps]$ tar -zxvf apache-flume-1.8.0-bin.tar.gz [hadoop@hadoop01 apps]$ cd apache-flume-1.8.0-bin/[hadoop@hadoop01 apache-flume-1.8.0-bin]$ ll总用量 148drwxr-xr-x. 2 hadoop hadoop 62 1月 21 14:31 bin-rw-r--r--. 1 hadoop hadoop 81264 9月 15 20:26 CHANGELOGdrwxr-xr-x. 2 hadoop hadoop 127 1月 21 14:31 conf-rw-r--r--. 1 hadoop hadoop 5681 9月 15 20:26 DEVNOTES-rw-r--r--. 1 hadoop hadoop 2873 9月 15 20:26 doap_Flume.rdfdrwxr-xr-x. 10 hadoop hadoop 4096 9月 15 20:48 docsdrwxr-xr-x. 2 hadoop hadoop 8192 1月 21 14:31 lib-rw-r--r--. 1 hadoop hadoop 27663 9月 15 20:26 LICENSE-rw-r--r--. 1 hadoop hadoop 249 9月 15 20:26 NOTICE-rw-r--r--. 1 hadoop hadoop 2483 9月 15 20:26 README.md-rw-r--r--. 1 hadoop hadoop 1588 9月 15 20:26 RELEASE-NOTESdrwxr-xr-x. 2 hadoop hadoop 68 1月 21 14:31 tools[hadoop@hadoop01 apache-flume-1.8.0-bin]$ 3.创建软连接【此步骤可省略】1[root@hadoop01 bin]# ln -s /home/hadoop/apps/apache-flume-1.8.0-bin /usr/local/flume 4.配置环境变量编辑 /etc/profile文件，增加以下内容： 12export FLUME_HOME=/usr/local/flumeexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$&#123;ZOOKEEPER_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;HIVE_HOME&#125;/bin:$&#123;FLUME_HOME&#125;/bin 4.启动flume使用example.conf 配置文件启动一个实例123456789101112a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sinks.k1.type = loggera1.sinks.k1.channel = c1 启动命令如下： 123[root@hadoop01 conf]# pwd/home/hadoop/apps/apache-flume-1.8.0-bin/conf[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console 启动成功后如下图所示： 1234567891011........略18/01/27 18:17:25 INFO node.AbstractConfigurationProvider: Channel c1 connected to [r1, k1]18/01/27 18:17:25 INFO node.Application: Starting new configuration:&#123; sourceRunners:&#123;r1=EventDrivenSourceRunner: &#123; source:org.apache.flume.source.NetcatSource&#123;name:r1,state:IDLE&#125; &#125;&#125; sinkRunners:&#123;k1=SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@20470f counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;c1=org.apache.flume.channel.MemoryChannel&#123;name: c1&#125;&#125; &#125;18/01/27 18:17:25 INFO node.Application: Starting Channel c118/01/27 18:17:25 INFO node.Application: Waiting for channel: c1 to start. Sleeping for 500 ms18/01/27 18:17:25 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.18/01/27 18:17:25 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started18/01/27 18:17:26 INFO node.Application: Starting Sink k118/01/27 18:17:26 INFO node.Application: Starting Source r118/01/27 18:17:26 INFO source.NetcatSource: Source starting18/01/27 18:17:26 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444] 使用telnet发送数据 12345678[root@hadoop01 apps]# telnet localhost 44444Trying ::1...telnet: connect to address ::1: Connection refusedTrying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.Are you OK ?OK 控制台打印如下： 12Impl[/127.0.0.1:44444]18/01/27 18:21:00 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 41 72 65 20 79 6F 75 20 4F 4B 20 3F 0D Are you OK ?. &#125; 如无法使用telnet，请先安装telnet工具 1[root@hadoop01 apps]# yum -y install telnet 三、Flume实践1.Source组件清单 Source：对接各种外部数据源，将收集到的事件发送到Channel中，一个source可以向多个channel发送event，Flume内置非常丰富的Source，同时用户可以自定义Source Source类型 Type 用途 Avro Source avro 启动一个Avro Server，可与上一级Agent连接 HTTP Source http 启动一个HttpServer Exec Source exec 执行unix command，获取标准输出，如tail -f Taildir Source TAILDIR 监听目录或文件 Spooling Directory Source spooldir 监听目录下的新增文件 Kafka Source org.apache.flume.sourc e.kafka.KafkaSource 读取Kafka数据 JMS Source jms 从JMS源读取数据 2.avro Source Agent 和Exec Source Agent 配置一个avroagent，avrosource.conf 配置文件如下： 1234567891011121314//avrosource.confavroagent.sources = r1avroagent.channels = c1avroagent.sinks = k1 avroagent.sources.r1.type = avroavroagent.sources.r1.bind = 192.168.43.20avroagent.sources.r1.port = 8888avroagent.sources.r1.threads= 3avroagent.sources.r1.channels = c1avroagent.channels.c1.type = memoryavroagent.channels.c1.capacity = 10000 avroagent.channels.c1.transactionCapacity = 1000avroagent.sinks.k1.type = loggeravroagent.sinks.k1.channel = c1 启动一个avrosource的agent1[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file avrosource.conf --name avroagent -Dflume.root.logger=INFO,console 启动成功入下图所示： 123456789...略18/01/27 18:46:36 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.18/01/27 18:46:36 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started18/01/27 18:46:36 INFO node.Application: Starting Sink k118/01/27 18:46:36 INFO node.Application: Starting Source r118/01/27 18:46:36 INFO source.AvroSource: Starting Avro source r1: &#123; bindAddress: 192.168.43.20, port: 8888 &#125;...18/01/27 18:46:37 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.18/01/27 18:46:37 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started18/01/27 18:46:37 INFO source.AvroSource: Avro source r1 started 配置一个execAgent，实现与sourceAgent实现串联，execsource.conf 配置文件如下： 12345678910111213execagent.sources = r1 execagent.channels = c1execagent.sinks = k1execagent.sources.r1.type = exec execagent.sources.r1.command = tail -F /home/hadoop/apps/flume/execsource/exectest.logexecagent.sources.r1.channels = c1execagent.channels.c1.type = memoryexecagent.channels.c1.capacity = 10000 execagent.channels.c1.transactionCapacity = 1000execagent.sinks.k1.type = avroexecagent.sinks.k1.channel = c1execagent.sinks.k1.hostname = 192.168.43.20execagent.sinks.k1.port = 8888 启动一个execAgent,并实现execagent监控文件变化，sourceAgent接收变化内容 启动 execAgent1[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file execsource.conf --name execagent 启动成功如下下图所示： 1234518/01/27 18:58:43 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started18/01/27 18:58:43 INFO sink.AbstractRpcSink: Rpc sink k1: Building RpcClient with hostname: 192.168.43.20, port: 888818/01/27 18:58:43 INFO sink.AvroSink: Attempting to create Avro Rpc client.18/01/27 18:58:43 WARN api.NettyAvroRpcClient: Using default maxIOWorkers18/01/27 18:58:44 INFO sink.AbstractRpcSink: Rpc sink k1 started. 在execAgent监控的文件下写入内容，观察sourceagent是否接收到变化内容 12345[root@hadoop01 execsource]# echo 222 &gt; exectest.log [root@hadoop01 execsource]# echo 5555 &gt;&gt; exectest.log [root@hadoop01 execsource]# cat exectest.log 2225555 在sourceagent控制打印台下查看监控消息如下： 1218/01/27 18:58:50 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 31 32 33 123 &#125;18/01/27 18:59:55 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 35 35 35 35 5555 &#125; 则说明2个串联agent传递信息成功。说明:avroagent 配置文件配置项起始名称需要与服务启动 -name 名称相一致。 3.Source组件- Spooling Directory Source 配置一个Spooling Directory Source ,spooldirsource.conf 配置文件内容如下： 123456789101112a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = spooldira1.sources.r1.channels = c1a1.sources.r1.spoolDir = /home/hadoop/apps/flume/spoolDira1.sources.r1.fileHeader = truea1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 1000a1.sinks.k1.type = loggera1.sinks.k1.channel = c1 /home/hadoop/apps/flume/spoolDir 必须已经创建且具有用户读写权限。 启动 SpoolDirsourceAgent 1[hadoop@hadoop01 conf]$ flume-ng agent --conf conf --conf-file spooldirsource.conf --name a1 -Dflume.root.logger=INFO,console 在spoolDir文件夹下创建文件并写入文件内容，观察控制台消息： 1218/01/28 17:06:54 INFO avro.ReliableSpoolingFileEventReader: Preparing to move file /home/hadoop/apps/flume/spoolDir/test to /home/hadoop/apps/flume/spoolDir/test.COMPLETED18/01/28 17:06:55 INFO sink.LoggerSink: Event: &#123; headers:&#123;file=/home/hadoop/apps/flume/spoolDir/test&#125; body: 32 32 32 222 &#125; 此时监测到SpoolDirSourceAgent 可以监控到文件变化。值得说明的是：Spooling Directory Source Agent 并不能监听子级文件夹的文件变化,也不支持已存在的文件更新数据变化. 4.Source组件- Kafka Source 配置一个Kafa Source , kafasource.conf 配置文件内容如下： 123456789101112131415kafkasourceagent.sources = r1kafkasourceagent.channels = c1kafkasourceagent.sinks = k1kafkasourceagent.sources.r1.type = org.apache.flume.source.kafka.KafkaSource kafkasourceagent.sources.r1.channels = c1 kafkasourceagent.sources.r1.batchSize = 100kafkasourceagent.sources.r1.batchDurationMillis = 1000kafkasourceagent.sources.r1.kafka.bootstrap.servers = 192.168.43.22:9092,192.168.43.23:9092,192.168.43.24:9092kafkasourceagent.sources.r1.kafka.topics = flumetopictest1kafkasourceagent.sources.r1.kafka.consumer.group.id = flumekafkagroupidkafkasourceagent.channels.c1.type = memorykafkasourceagent.channels.c1.capacity = 10000 kafkasourceagent.channels.c1.transactionCapacity = 1000kafkasourceagent.sinks.k1.type = loggerkafkasourceagent.sinks.k1.channel = c1 首先启动3个节点的kafka节点服务，在每个kafka节点执行，以后台方式运行 1[root@hadoop03 bin]# ./kafka-server-start.sh -daemon ../config/server.properties 在kafka节点上创建一个配置好的Topic flumetoptest1,命令如下： 12[root@hadoop03 bin]# ./kafka-topics.sh --create --zookeeper 192.168.43.20:2181 --replication-factor 1 --partitions 3 --topic flumetopictest1Created topic &quot;flumetopictest1&quot;. 创建成功后，启动一个kafka Source Agent，命令如下： 1[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file kafkasource.conf --name kafkasourceagent -Dflume.root.logger=INFO,console 创建一个Kafka 生产者，进行消息发送1root@hadoop03 bin]# ./kafka-console-producer.sh --broker-list 192.168.43.22:9092,192.168.43.23:9092 --topic flumetopictest1 发送消息，此时kafka 就可以接收到消息： 1218/02/03 20:36:57 INFO sink.LoggerSink: Event: &#123; headers:&#123;topic=flumetopictest1, partition=2, timestamp=1517661413068&#125; body: 31 32 33 31 33 32 32 31 12313221 &#125;18/02/03 20:37:09 INFO sink.LoggerSink: Event: &#123; headers:&#123;topic=flumetopictest1, partition=1, timestamp=1517661428930&#125; body: 77 69 20 61 69 79 6F 75 08 08 08 wi aiyou... &#125; 5.Source 组件 -Taildir source 监听一个文件夹或者文件，通过正则表达式匹配需要监听的 数据源文件，Taildir Source通过将监听的文件位置写入到文件中来实现断点续传，并且能够保证没有重复数据的读取. 重要参数type：source类型TAILDIRpositionFile：保存监听文件读取位置的文件路径idleTimeout：关闭空闲文件延迟时间，如果有新的记录添加到已关闭的空闲文件taildir srouce将继续打开该空闲文件，默认值120000毫秒writePosInterval：向保存读取位置文件中写入读取文件位置的时间间隔，默认值3000毫秒batchSize：批量写入channel最大event数，默认值100maxBackoffSleep：每次最后一次尝试没有获取到监听文件最新数据的最大延迟时 间，默认值5000毫秒cachePatternMatching：对于监听的文件夹下通过正则表达式匹配的文件可能数量 会很多，将匹配成功的监听文件列表和读取文件列表的顺序都添加到缓存中，可以提高性能，默认值truefileHeader ：是否添加文件的绝对路径到event的header中，默认值falsefileHeaderKey：添加到event header中文件绝对路径的键值，默认值filefilegroups：监听的文件组列表，taildirsource通过文件组监听多个目录或文件filegroups.：文件正则表达式路径或者监听指定文件路径channels：Source对接的Channel名称 配置一个taildir Source,具体taildirsource.conf 配置文件内容如下： 12345678910111213taildiragent.sources=r1taildiragent.channels=c1taildiragent.sinks=k1taildiragent.sources.r1.type=TAILDIRtaildiragent.sources.r1.positionFile=/home/hadoop/apps/flume/taildir/position/taildir_position.jsontaildiragent.sources.r1.filegroups=f1 f2taildiragent.sources.r1.filegroups.f1=/home/hadoop/apps/flume/taildir/test1/test.logtaildiragent.sources.r1.filegroups.f2=/home/hadoop/apps/flume/taildir/test2/.*log.*taildiragent.sources.r1.channels=c1taildiragent.channels.c1.type=memorytaildiragent.channels.c1.transcationCapacity=1000taildiragent.sinks.k1.type=loggertaildiragent.sinks.k1.channel=c1 启动一个taildirSource agent ,代码如下： 1[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file taildirsource.conf --name taildiragent -Dflume.root.logger=INFO,console 开始在test1和test2文件夹写入文件，观察agent消息接收。 6.Channel组件 Channel：Channel被设计为event中转暂存区，存储Source 收集并且没有被Sink消费的event ，为了平衡Source收集 和Sink读取数据的速度，可视为Flume内部的消息队列。 Channel是线程安全的并且具有事务性，支持source写失 败重复写和sink读失败重复读等操作 常用的Channel类型有：Memory Channel、File Channel、Kafka Channel、JDBC Channel等7.Channel组件- Memory Channel Memory Channel：使用内存作为Channel，Memory Channel读写速度 快，但是存储数据量小，Flume进程挂掉、服务器停机或者重启都会 导致数据丢失。部署Flume Agent的线上服务器内存资源充足、不关 心数据丢失的场景下可以使用关键参数： 123456type ：channel类型memorycapacity ：channel中存储的最大event数，默认值100transactionCapacity ：一次事务中写入和读取的event最大数，默认值100。keep-alive：在Channel中写入或读取event等待完成的超时时间，默认值3秒byteCapacityBufferPercentage：缓冲空间占Channel容量（byteCapacity）的百分比，为event中的头信息保留了空间，默认值20（单位百分比）byteCapacity ：Channel占用内存的最大容量，默认值为Flume堆内存的80% 8. Channel组件- File Channel File Channel：将event写入到磁盘文件中，与Memory Channel相比存 储容量大，无数据丢失风险。 File Channle数据存储路径可以配置多磁盘文件路径，提高写入文件性能 Flume将Event顺序写入到File Channel文件的末尾，在配置文件中通过设置maxFileSize参数设置数据文件大小上限 当一个已关闭的只读数据文件中的Event被完全读取完成，并且Sink已经提交读取完成的事务，则Flume将删除存储该数据文件 通过设置检查点和备份检查点在Agent重启之后能够快速将File Channle中的数据按顺序回放到内存中关键参数如下： 1234567891011type：channel类型为file checkpointDir：检查点目录，默认在启动flume用户目录下创建，建 议单独配置磁盘路径 useDualCheckpoints：是否开启备份检查点，默认false，建议设置为true开启备份检查点，备份检查点的作用是当Agent意外出错导致写 入检查点文件异常，在重新启动File Channel时通过备份检查点将数据回放到内存中，如果不开启备份检查点，在数据回放的过程中发现检查点文件异常会对所数据进行全回放，全回放的过程相当耗时 backupCheckpointDir：备份检查点目录，最好不要和检查点目录在同 一块磁盘上 checkpointInterval：每次写检查点的时间间隔，默认值30000毫秒 dataDirs：数据文件磁盘存储路径，建议配置多块盘的多个路径，通过磁盘的并行写入来提高file channel性能，多个磁盘路径用逗号隔开transactionCapacity：一次事务中写入和读取的event最大数，默认值 10000maxFileSize：每个数据文件的最大大小，默认值：2146435071字节minimumRequiredSpace：磁盘路径最小剩余空间，如果磁盘剩余空 间小于设置值，则不再写入数据capacity：file channel可容纳的最大event数keep-alive：在Channel中写入或读取event等待完成的超时时间，默认值3秒 配置一个FileChannel,filechannel.conf 的配置内容如下： 1234567891011121314a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.channels = c1a1.channels.c1.type = filea1.channels.c1.dataDirs = /home/hadoop/apps/flume/filechannel/dataa1.channels.c1.checkpointDir = /home/hadoop/apps/flume/filechannel/checkpoint a1.channels.c1.useDualCheckpoints = truea1.channels.c1.backupCheckpointDir = /home/hadoop/apps/flume/filechannel/backupa1.sinks.k1.type = loggera1.sinks.k1.channel = c1 启动一个FileChannel,启动命令如下： 1[root@hadoop01 bin]# flume-ng agent --conf conf --conf-file filechannle.conf --name a1 -Dflume.root.logger=INFO,console 向配置文件端口44444发送数据，观察Channel记录情况 1telnet localhost asdfasd 此时可以观察到控制台打印监控结果 1234567818/02/04 21:15:44 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 61 64 66 61 64 66 61 64 66 61 73 66 0D adfadfadfasf. &#125;18/02/04 21:15:48 INFO file.EventQueueBackingStoreFile: Start checkpoint for /home/hadoop/apps/flume/filechannel/checkpoint/checkpoint, elements to sync = 118/02/04 21:15:48 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1517749968978, queueSize: 0, queueHead: 018/02/04 21:15:48 INFO file.EventQueueBackingStoreFile: Attempting to back up checkpoint.18/02/04 21:15:48 INFO file.Serialization: Skipping in_use.lock because it is in excludes set18/02/04 21:15:48 INFO file.Serialization: Deleted the following files: , checkpoint, checkpoint.meta, inflightputs, inflighttakes.18/02/04 21:15:48 INFO file.Log: Updated checkpoint for file: /home/hadoop/apps/flume/filechannel/data/log-2 position: 170 logWriteOrderID: 151774996897818/02/04 21:15:49 INFO file.EventQueueBackingStoreFile: Checkpoint backup completed. 9.Channel组件- Kafka ChannelKafka Channel：将分布式消息队列kafka作为channel相对于Memory Channel和File Channel存储容量更大、 容错能力更强，弥补了其他两种Channel的短板，如果合理利用Kafka的性能，能够达到事半功倍的效果。关键参数如下： 1234567type：Kafka Channel类型org.apache.flume.channel.kafka.KafkaChannelkafka.bootstrap.servers：Kafka broker列表，格式为ip1:port1, ip2:port2…，建 议配置多个值提高容错能力，多个值之间用逗号隔开kafka.topic：topic名称，默认值“flume-channel”kafka.consumer.group.id：Consumer Group Id，全局唯一parseAsFlumeEvent：是否以Avro FlumeEvent模式写入到Kafka Channel中， 默认值true，event的header信息与event body都写入到kafka中pollTimeout：轮询超时时间，默认值500毫秒kafka.consumer.auto.offset.reset：earliest表示从最早的偏移量开始拉取，latest表示从最新的偏移量开始拉取，none表示如果没有发现该Consumer组之前拉 取的偏移量则抛异常 配置一个KafakChannel， kafkachannel.conf 配置内容如下： 12345678910111213a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.channels = c1a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannela1.channels.c1.kafka.bootstrap.servers = 192.168.43.22:9092,192.168.43.23:9092a1.channels.c1.kafka.topic = flumechannel2a1.channels.c1.kafka.consumer.group.id = flumecgtest1a1.sinks.k1.type = loggera1.sinks.k1.channel = c1 启动kafak服务，创建一个kafka主题，命令如下： 12[root@hadoop03 bin]# ./kafka-server-start.sh -daemon ../config/server.properties[root@hadoop03 bin]# ./kafka-topics.sh --create --zookeeper 192.168.43.20:2181 --replication-factor 1 --partitions 3 --topic flumechannel2 查看创建的主题信息1234[root@hadoop03 bin]# ./kafka-topics.sh --list --zookeeper 192.168.43.20:2181__consumer_offsetsflumechannel2topicnewtest1 启动kafka agent,使用telnet发送数据 12345678910[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file kafkachannel.conf --name a1 -Dflume.root.logger=INFO,console[root@hadoop01 flume]# clear[root@hadoop01 flume]# telnet localhost 44444 Trying ::1...telnet: connect to address ::1: Connection refusedTrying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.abcOK 监听信息如下： 118/02/04 21:39:33 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 61 62 63 0D abc. &#125; 10.Sink组件 Sink：从Channel消费event，输出到外部存储，或者输出到下一个阶段的agent 一个Sink只能从一个Channel中消费event 当Sink写出event成功后，就会向Channel提交事务。Sink 事务提交成功，处理完成的event将会被Channel删除。否 则Channel会等待Sink重新消费处理失败的event Flume提供了丰富的Sink组件，如Avro Sink、HDFS Sink、Kafka Sink、File Roll Sink、HTTP Sink等11.Sink组件- Avro Sink Avro Sink常用于对接下一层的Avro Source，通过发送RPC请求将Event发送到下一层的Avro Source 为了减少Event传输占用大量的网络资源， Avro Sink提供了端到端的批量压缩数据传输 关键参数说明12345678type：Sink类型为avro。hostname：绑定的目标Avro Souce主机名称或者IPport：绑定的目标Avro Souce端口号batch-size：批量发送Event数，默认值100compression-type：是否使用压缩，如果使用压缩设则值为“deflate”， Avro Sink设置了压缩那么Avro Source也应设置相同的 压缩格式，目前支持zlib压缩，默认值nonecompression-level：压缩级别，0表示不压缩，从1到9数字越大压缩效果越好，默认值6 12.Sink组件- HDFS Sink HDFS Sink将Event写入到HDFS中持久化存储 HDFS Sink提供了强大的时间戳转义功能，根据Event头信息中的 timestamp时间戳信息转义成日期格式，在HDFS中以日期目录分层存储 关键参数信息说明如下：1234567type：Sink类型为hdfs。hdfs.path：HDFS存储路径，支持按日期时间分区。hdfs.filePrefix：Event输出到HDFS的文件名前缀，默认前缀FlumeDatahdfs.fileSuffix：Event输出到HDFS的文件名后缀hdfs.inUsePrefix：临时文件名前缀hdfs.inUseSuffix：临时文件名后缀，默认值.tmphdfs.rollInterval：HDFS文件滚动生成时间间隔，默认值30秒，该值设置 为0表示文件不根据时间滚动生成 配置一个hdfsink.conf文件，配置内容如下： 1234567891011121314151617181920212223a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = timestampa1.sources.r1.interceptors.i1.preserveExisting = falsea1.sources.r1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000 a1.channels.c1.transactionCapacity = 1000a1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1a1.sinks.k1.hdfs.path = /data/flume/%Y%m%da1.sinks.k1.hdfs.filePrefix = hdfssinka1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.writeFormat = Texta1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 1a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.callTimeout = 60000 启动一个hdfssink agent，命令如下： 1[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file hdfssink.conf --name a1 -Dflume.root.logger=INFO,console 使用telnet 向44444发送数据，观察数据写入结果 12345678910[hadoop@hadoop01 root]$ telnet localhost 44444Trying ::1...telnet: connect to address ::1: Connection refusedTrying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.abcOK2323444OK 此时控制台打印，在HDFS文件系统生成一个临时文件 123458/02/04 22:41:52 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false18/02/04 22:41:52 INFO hdfs.BucketWriter: Creating /data/flume/20180204/hdfssink.1517755312242.tmp18/02/04 22:42:24 INFO hdfs.BucketWriter: Closing /data/flume/20180204/hdfssink.1517755312242.tmp18/02/04 22:42:24 INFO hdfs.BucketWriter: Renaming /data/flume/20180204/hdfssink.1517755312242.tmp to /data/flume/20180204/hdfssink.151775531224218/02/04 22:42:24 INFO hdfs.HDFSEventSink: Writer callback called. 值得注意的是：请使用hadoop用户来执行agent的创建和消息的发送，避免因权限导致HDFS文件无法写入 13.Sink组件- Kafka SinkFlume通过KafkaSink将Event写入到Kafka指定的主题中主要参数说明如下： 123456type：Sink类型，值为KafkaSink类路径 org.apache.flume.sink.kafka.KafkaSink。kafka.bootstrap.servers：Broker列表，定义格式host:port，多个Broker之间用逗号隔开，可以配置一个也可以配置多个，用于Producer发现集群中的Broker，建议配置多个，防止当个Broker出现问题连接 失败。kafka.topic：Kafka中Topic主题名称，默认值flume-topic。flumeBatchSize：Producer端单次批量发送的消息条数，该值应该根据实际环境适当调整，增大批量发送消息的条数能够在一定程度上提高性能，但是同时也增加了延迟和Producer端数据丢失的风险。 默认值100。kafka.producer.acks：设置Producer端发送消息到Borker是否等待接收Broker返回成功送达信号。0表示Producer发送消息到Broker之后不需要等待Broker返回成功送达的信号，这种方式吞吐量高，但是存 在数据丢失的风险。1表示Broker接收到消息成功写入本地log文件后向Producer返回成功接收的信号，不需要等待所有的Follower全部同步完消息后再做回应，这种方式在数据丢失风险和吞吐量之间做了平衡。all（或者-1）表示Broker接收到Producer的消息成功写入本 地log并且等待所有的Follower成功写入本地log后向Producer返回成功接收的信号，这种方式能够保证消息不丢失，但是性能最差。默 认值1。useFlumeEventFormat：默认值false，Kafka Sink只会将Event body内 容发送到Kafka Topic中。如果设置为true，Producer发送到KafkaTopic中的Event将能够保留Producer端头信息 配置一个kafkasink.conf,具体配置内容如下： 12345678910111213141516a1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000 a1.channels.c1.transactionCapacity = 1000a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.channel = c1a1.sinks.k1.kafka.topic = FlumeKafkaSinkTopic1a1.sinks.k1.kafka.bootstrap.servers = 192.168.43.22:9092,192.168.43.23:9092a1.sinks.k1.kafka.flumeBatchSize = 100a1.sinks.k1.kafka.producer.acks = 1 启动kafka Broker节点22和Broker节点23 1[root@hadoop03 bin]# ./kafka-server-start.sh -daemon ../config/server.properties 按配置文件创建主题信息12[root@hadoop03 bin]# ./kafka-topics.sh --create --zookeeper 192.168.43.20:2181 --replication-factor 1 --partitions 3 --topic FlumeKafkaSinkTopic1Created topic &quot;FlumeKafkaSinkTopic1&quot;. 启动一个kafkasink agent，启动命令如下：1[root@hadoop01 conf]# flume-ng agent --conf conf --conf-file kafkasink.conf --name a1 &gt;/dev/null 2&gt;&amp;1 &amp; 14.Interceptor拦截器 Source将event写入到Channel之前调用拦截器 Source和Channel之间可以有多个拦截器，不同的拦截器使用不同的 规则处理Event 可选、轻量级、可插拔的插件 通过实现Interceptor接口实现自定义的拦截器 内置拦截器：Timestamp Interceptor、Host Interceptor、UUID Interceptor、Static Interceptor、Regex Filtering Interceptor等15.Timestamp Interceptor Flume使用时间戳拦截器在event头信息中添加时间戳信息， Key为timestamp，Value为拦截器拦截Event时的时间戳 头信息时间戳的作用，比如HDFS存储的数据采用时间分区存储，Sink可以根据Event头信息中的时间戳将Event按照时间分区写入到 HDFS 关键参数说明： type:拦截器类型为timestamp preserveExisting：如果头信息中存在timestamp时间戳信息是否保留原来的时间戳信息，true保留，false使用新的时间戳替换已经存在的时间戳，默认值为false16.Host Interceptor Flume使用主机戳拦截器在Event头信息中添加主机名称或者IP 主机拦截器的作用：比如Source将Event按照主机名称写入到不同的Channel中便于后续的Sink对不同Channnel中的数据分开处理 关键参数说明： type:拦截器类型为host preserveExisting：如果头信息中存在timestamp时间戳信息是否保留原来的时间戳信息，true保留，false使用新的时间戳替换已经存在的时间戳，默认值为false useIP：是否使用IP作为主机信息写入都信息，默认值为false hostHeader：设置头信息中主机信息的Key，默认值为host17.Host InterceptorStatic Interceptor Flume使用static interceptor静态拦截器在evetn头信息添加静态信息 关键参数说明： type:拦截器类型为static preserveExisting：如果头信息中存在timestamp时间戳信息是否保留原来的时间戳信息，true保留，false使用新的时间戳替换已经 存在的时间戳，默认值为false key：头信息中的键 value：头信息中键对应的值18.Selector选择器 Source将event写入到Channel之前调用拦截器，如果配置了Interceptor拦截器，则Selector在拦截器全部处理完之后调用。通过selector决定event写入Channel的方式 内置Replicating Channel Selector复制Channel选择器、 Multiplexing Channel Selector复用Channel选择器19.Replicating Channel Selector 如果Channel选择器没有指定，默认是Replicating Channel Selector。即一个Source以复制的方式将一个event同时写入到多个Channel中，不同的Sink可以从不同的Channel中获取相同的event。 关键参数说明： selector.type：Channel选择器类型为replicating selector.optional：定义可选Channel，当写入event到可选Channel失败时，不会向Source抛出异常，继续执行。多个可选Channel之 间用空格隔开 一个source将一个event拷贝到多个channel，通过不同的sink消费不同的channel，将相同的event输出到不同的地方配置文件：replicating_selector.conf 12345678910111213141516171819202122232425262728293031a1.sources = r1a1.channels = c1 c2a1.sinks = k1 k2#定义sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444#设置复制选择器a1.sources.r1.selector.type = replicating#设置required channela1.sources.r1.channels = c1 c2#设置channel c1a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 1000#设置channel c2a1.channels.c2.type = memory a1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 1000#设置kafka sinka1.sinks.k1.channel = c1a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = FlumeSelectorTopic1a1.sinks.k1.kafka.bootstrap.servers = 192.168.43.22:9092,192.168.23.103:9092a1.sinks.k1.kafka.flumeBatchSize = 5a1.sinks.k1.kafka.producer.acks = 1#设置file sinka1.sinks.k2.channel = c2a1.sinks.k2.type = file_rolla1.sinks.k2.sink.directory = /home/hadoop/apps/flume/selectora1.sinks.k2.sink.rollInterval = 60 分别写入到kafka和文件中 创建主题FlumeKafkaSinkTopic1 1bin/kafka-topics.sh --create --zookeeper 192.168.183.100:2181 --replication-factor 1 --partitions 3 --topic FlumeSelectorTopic1 启动flume agent 1bin/flume-ng agent --conf conf --conf-file conf/replicating_selector.conf --name a1 使用telnet发送数据1telnet localhost 44444 查看/home/hadoop/apps/flume/selector路径下的数据 查看kafka FlumeSelectorTopic1主题数据 1bin/kafka-console-consumer.sh --zookeeper 192.168.183.100:2181 --from-beginning --topic FlumeSelectorTopic1 20.Multiplexing Channel Selector-Multiplexing Channel Selector多路复用选择器根据event的头信息中不同键值数据来判断Event应该被写入到哪个Channel中 三种级别的Channel，分别是必选channle、可选channel、默认channel 关键参数说明： 12345selector.type：Channel选择器类型为multiplexingselector.header：设置头信息中用于检测的headerNameselector.default：默认写入的Channel列表selector.mapping.*：headerName对应的不同值映射的不同Channel列表selector.optional：可选写入的Channel列表 配置文件multiplexing_selector.conf、avro_sink1.conf、avro_sink2.conf、avro_sink3.conf向不同的avro_sink对应的配置文件的agent发送数据，不同的avro_sink配置文件通过static interceptor在event头信息中写入不同的静态数据multiplexing_selector根据event头信息中不同的静态数据类型分别发送到不同的目的地 multiplexing_selector.conf12345678910111213141516171819202122232425262728293031323334353637a3.sources = r1a3.channels = c1 c2 c3a3.sinks = k1 k2 k3a3.sources.r1.type = avroa3.sources.r1.bind = 192.168.183.100a3.sources.r1.port = 8888a3.sources.r1.threads= 3#设置multiplexing selectora3.sources.r1.selector.type = multiplexinga3.sources.r1.selector.header = logtype#通过header中logtype键对应的值来选择不同的sinka3.sources.r1.selector.mapping.ad = c1a3.sources.r1.selector.mapping.search = c2a3.sources.r1.selector.default = c3a3.sources.r1.channels = c1 c2 c3a3.channels.c1.type = memorya3.channels.c1.capacity = 10000a3.channels.c1.transactionCapacity = 1000a3.channels.c2.type = memorya3.channels.c2.capacity = 10000a3.channels.c2.transactionCapacity = 1000a3.channels.c3.type = memorya3.channels.c3.capacity = 10000a3.channels.c3.transactionCapacity = 1000#分别设置三个sink的不同输出a3.sinks.k1.type = file_rolla3.sinks.k1.channel = c1a3.sinks.k1.sink.directory = /home/hadoop/apps/flume/multiplexing/k11a3.sinks.k1.sink.rollInterval = 60a3.sinks.k2.channel = c2a3.sinks.k2.type = file_rolla3.sinks.k2.sink.directory = /home/hadoop/apps/flume/multiplexing/k12a3.sinks.k2.sink.rollInterval = 60a3.sinks.k3.channel = c3a3.sinks.k3.type = file_rolla3.sinks.k3.sink.directory = /home/hadoop/apps/flume/multiplexing/k13a3.sinks.k3.sink.rollInterval = 60 avro_sink1.conf 12345678910111213141516171819agent1.sources = r1agent1.channels = c1agent1.sinks = k1agent1.sources.r1.type = netcatagent1.sources.r1.bind = localhostagent1.sources.r1.port = 44444agent1.sources.r1.interceptors = i1agent1.sources.r1.interceptors.i1.type = staticagent1.sources.r1.interceptors.i1.key = logtypeagent1.sources.r1.interceptors.i1.value = adagent1.sources.r1.interceptors.i1.preserveExisting = falseagent1.sources.r1.channels = c1agent1.channels.c1.type = memoryagent1.channels.c1.capacity = 10000 agent1.channels.c1.transactionCapacity = 1000agent1.sinks.k1.type = avroagent1.sinks.k1.channel = c1agent1.sinks.k1.hostname = 192.168.183.100agent1.sinks.k1.port = 8888 avro_sink2.conf 12345678910111213141516171819agent2.sources = r1agent2.channels = c1agent2.sinks = k1agent2.sources.r1.type = netcatagent2.sources.r1.bind = localhostagent2.sources.r1.port = 44445agent2.sources.r1.interceptors = i1agent2.sources.r1.interceptors.i1.type = staticagent2.sources.r1.interceptors.i1.key = logtypeagent2.sources.r1.interceptors.i1.value = searchagent2.sources.r1.interceptors.i1.preserveExisting = falseagent2.sources.r1.channels = c1agent2.channels.c1.type = memoryagent2.channels.c1.capacity = 10000 agent2.channels.c1.transactionCapacity = 1000agent2.sinks.k1.type = avroagent2.sinks.k1.channel = c1agent2.sinks.k1.hostname = 192.168.183.100agent2.sinks.k1.port = 8888 avro_sink3.conf 12345678910111213141516171819agent3.sources = r1agent3.channels = c1agent3.sinks = k1agent3.sources.r1.type = netcatagent3.sources.r1.bind = localhostagent3.sources.r1.port = 44446agent3.sources.r1.interceptors = i1agent3.sources.r1.interceptors.i1.type = staticagent3.sources.r1.interceptors.i1.key = logtypeagent3.sources.r1.interceptors.i1.value = otheragent3.sources.r1.interceptors.i1.preserveExisting = falseagent3.sources.r1.channels = c1agent3.channels.c1.type = memoryagent3.channels.c1.capacity = 10000 agent3.channels.c1.transactionCapacity = 1000agent3.sinks.k1.type = avroagent3.sinks.k1.channel = c1agent3.sinks.k1.hostname = 192.168.183.100agent3.sinks.k1.port = 8888 在/home/hadoop/apps/flume/multiplexing目录下分别创建看k1 k2 k3目录 1234bin/flume-ng agent --conf conf --conf-file conf/multiplexing_selector.conf --name a3 -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf --conf-file conf/avro_sink1.conf --name agent1 &gt;/dev/null 2&gt;&amp;1 &amp;bin/flume-ng agent --conf conf --conf-file conf/avro_sink2.conf --name agent2 &gt;/dev/null 2&gt;&amp;1 &amp;bin/flume-ng agent --conf conf --conf-file conf/avro_sink3.conf --name agent3 &gt;/dev/null 2&gt;&amp;1 &amp; 使用telnet发送数据telnet localhost 44444 21.Sink Processor Sink Processor协调多个sink间进行load balance和fail over Default Sink Processor只有一个sink，无需创建Sink Processor Sink Group：将多个sink放到一个组内，要求组内一个sink消费channel Load-Balancing Sink Processor（负载均衡处理器）round_robin(默认)或 random Failover Sink Processor（容错处理器）可定义一个sink优先级列表，根据优先级选择使用的sink22.Load-Balancing Sink Processor关键参数说明： 12345sinks：sink组内的子Sink，多个子sink之间用空格隔开processor.type：设置负载均衡类型load_balanceprocessor.backoff：设置为true时，如果在系统运行过程中执行的Sink失败，会将失败的Sink放进一个冷却池中。默认值falseprocessor.selector.maxTimeOut：失败sink在冷却池中最大驻留时间，默认值30000msprocessor.selector：负载均衡选择算法，可以使用轮询“round_robin”、随机“random”或者是继承AbstractSinkSelector类的自定义负载均衡实现类 示例 23.Failover Sink Processor关键参数说明：1234sinks：sink组内的子Sink，多个子sink之间用空格隔开processor.type：设置故障转移类型“failover”processor.priority.&lt;sinkName&gt;：指定Sink组内各子Sink的优先级别，优先级从高到低，数值越大优先级越高processor.maxpenalty：等待失败的Sink恢复的最长时间，默认值30000毫秒 示例 24.Failover应用场景 分布式日志收集场景 多个agent收集不同机器上相同类型的日志数据，为了保障高可用，采用分层部署，日志收集层Collector部署两个甚至多个，Agent通过Failover SinkProcessor实现其中任何一个collector挂掉不影响系统的日志收集服务 示例 总结 总结]]></content>
      <tags>
        <tag>flume</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql 操作笔记]]></title>
    <url>%2Fblogs%2F2018%2F02%2F02%2FMysql_%E6%93%8D%E4%BD%9C%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.登录1cd usr/local/mysql/bin ---&gt;./myql -uroot 2.创建数据库1CREATE DATABASE IF NOT EXISTS jxytest DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 3.为用户授权操作数据库1grant all on jxytest.* to &apos;yxy_jx&apos;@&apos;%&apos; identified by &apos;lzyljd_yxyjx_2017&apos; with grant option; 4.刷出权限1flush privileges; 查看Mysql最大文件导入大小1show VARIABLES like &apos;%max_allowed_packet%&apos;; 设置最大文件导入大小：1set global max_allowed_packet = 9*1024*1024*100; // 900M 5.慢查询1show status 显示系统状态参数 1.查询慢查询是否开启：123show global variables like &apos;%slow_query%&apos;;slow_query_log 为OFF 表示没有开启，mysql默认是没有开启慢查询日志记录功能的set slow_query_log=1 开启慢查询日志记录 2.查看慢查询时间1show variables like &apos;long_query_time&apos;; 默认为10秒钟，意识是大于10s才算慢查询 3.修改慢查询记录的时间1set long_query_time=1; 设置时间为1s 4.记录慢查询日志的次数1show status like &apos;slow_queries&apos;;]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop MapReduce优化和资源调度器]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FHadoop_MapReduce%E4%BC%98%E5%8C%96%E5%92%8C%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Hadoop Shuffle过程 1.Hadoop MapReduce Shuffle过程 Hadoop Shuffle过程 Map Shuffle过程图2 2.Shuffle过程要点记录 每个Map Task把输出结果写到内存中的环形缓冲区。 当内存环形缓冲区写入的数据量达到一定阈值时，后台线程会把 数据溢写到磁盘。 根据Partitioner，把数据写入到不同的partition 对于每个partition的数据进行排序 随着Map Task的不断运行，磁盘上的溢出文件越来越多 将这些溢出文件合并 对于一个partition下的不同分片，使用归并排序，同一分区内数据有序 Reduce Task通过网络远程拷贝MapTask的结果文件中的属于它的分区数据 合并所有已拷贝过来的数据文件 采用归并排序算法，对文件数据内容整理排序，将相同key的数据分 为一组，不同key之间有序 最终生成一个key对应一组值的数据集，一个key对应的一组数据会调用一次reduce方法3. Combinery优化总结 Combiner优化 Combiner调用的地方 MapTask的环形缓冲区向磁盘溢写文件之前调用Combiner Map阶段在合并本地多个文件写入一个大文件之前调用Combiner 使用Combiner的好处 减少Map Task输出数据量，由于临时结果写入到本地磁盘，所以能 够减少磁盘IO 减少Reduce-Map网络传输数据量，由于reduce需要远程通过网络从 Map拷贝数据，提高拷贝速度 应用场景 针对结果可以叠加的场景 SUM(YES) Average（NO） 设置方法（local reducer） job.setCombinerClass(WordCountReducer.class)4.YARN 资源调度器1. YARN-FIFO Scheduler将所有应用程序放入到一个队列中 先进入队里排在前面的程序先获得资源局限性 资源利用率低，无法交叉运行作业 不够灵活，比如紧急的作业无法插队，耗时长作业拖慢耗时短作业2. YARN-多队列分开调度器所有资源按照比例划分到不同的队列 每个队列可以实现单独的调度策略 优点 按照不同的资源使用情况将资源划分到不同队列 能够让更多的应用程序获得资源 使用灵活，资源利用率高 调度器 CapacityScheduler调度器 FairScheduler调度器 CapacityScheduler 由Yahoo开源，共享集群调度器 以队列方式组织作业 每个队列内部采用FIFO调度策略 每个队列分配一定比例资源 可限制每个用户使用资源量 CapacityScheduler.png CapacityScheduler 配置方法 在yarn-site.xml 设置使用CapacityScheduler调度器 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 在Hadoop配置文件目录下/usr/local/hadoop/etc/hadoop创建capacity-scheduler.xml,添加信息如下： 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt; &lt;value&gt;default,data-bi&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt; &lt;value&gt;80&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.bi.capacity&lt;/name&gt; &lt;value&gt;40&lt;/vaule&gt; &lt;/property&gt;&lt;/configuration&gt; 配置说明 capacity-scheduler.xml参数说明 capacity：队列占用的集群资源容量百分比，所有队列的容量 之和应小于100 maximum-capacity：由于存在资源共享，因此一个队列使用 的资源量可能超过其容量，而最多使用资源量可通过该参数 限制 配置完成无需重启YARN，使用管理命令刷新调度配置 bin/yarn rmadmin -refreshQueues FairScheduler 公平调度器的目的: 允许多用户共享集群资源。 允许短时的临时作业与长时作业共享集群资源 根据比例来管理集群资源，确保集群资源的有效利用’ FairScheduler配置方法在Hadoop配置目录下/usr/local/hadoop/etc/hadoop yarn-site.xml 增加如下信息：12345678910111213141516&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.fair.user-as-default-queue&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop/fair-scheduler.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 新建一个公平调度配置文件fair-scheduler.xml ，信息如下： 12345678&lt;allocations&gt; &lt;queue name=&quot;data_bi&quot;&gt; &lt;minResources&gt;8000 mb,4 vcores&lt;/minResources&gt; &lt;maxResources&gt;10000 mb, 6 vcores&lt;/maxResources&gt; &lt;maxRunningApps&gt;2&lt;/maxRunningApps&gt; &lt;weight&gt;1.0&lt;/weight&gt; &lt;/queue&gt;&lt;/allocations&gt; 上述配置以 data_bi 用户名作为公平调度的队列名称。 yarn-site.xml参数说明 yarn.resourcemanager.scheduler.class配置yarn使用的调度器类型 yarn.scheduler.fair.allocation.file配置公平调度器自定义配置文件路径，该文件每隔10秒就会被加载一次，这样就可以在集群运行过程中改变队列的配置 yarn.scheduler.fair.user-as-default-queue当应用程序未指定队列名时，是否指定用户名作为应用程序所在的队列名。如果设置为false或者未设置，所有 未知队列的应用程序将被提交到default队列中，默认值为true yarn.scheduler.fair.preemption如果一个队列占用的资源量少于最小资源量限制，是否启用资源抢占，默认false。抢占机制可以使其他队列的作业容器终止，从而使占用的资源让出，将资源分配给占用资源量少于最小资源量限制的队列 fair-scheduler.xml参数说明 queue name：配置队列名 minResources ：分配给该队列的最小资源量，设置格式为“X mb, Y vcores”，当调度策略属性schedulingPolicy的属性值是fair时，其cores值会被忽略，仅按照申请的内存大小来调度。 maxResources：分配给该队列的最大资源量。设置格式为“X mb, Y vcores”，当调度策略属性schedulingPolicy的属性值是fair时，其cores值会被忽略，仅按照申请的内存大小来调度。 maxRunningApps：最多同时运行的应用程序数目。通过限制该数目，可防止超量MapTask同时运行时产生的中间输出结果撑爆磁盘。 weight：标记了资源池的权重，当资源池中有任务等待，并且集群中有空闲资源时候，每个资源池可以根据权重获得不同比例的集群空闲资源，默认值是1]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS磁盘空间爆满的解决办法]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FCentOS%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E7%88%86%E6%BB%A1%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.查看磁盘具体占用情况12345678910[root@localhost gsidc]# df -hTFilesystem Type Size Used Avail Use% Mounted on/dev/sda3 xfs 92G 21G 72G 23% /devtmpfs devtmpfs 7.8G 0 7.8G 0% /devtmpfs tmpfs 7.8G 0 7.8G 0% /dev/shmtmpfs tmpfs 7.8G 129M 7.7G 2% /runtmpfs tmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup/dev/sda1 xfs 197M 192M 5.0M 98% /boottmpfs tmpfs 1.6G 12K 1.6G 1% /run/user/42tmpfs tmpfs 1.6G 0 1.6G 0% /run/user/1000 2.删除具体位置磁盘占用无用文件3. 查看磁盘删除情况1[root@localhost gsidc]# lsof |grep delete 4. 彻底释放磁盘空间1234567891011121314mysqld_sa 11317 root cwd DIR 8,6 0 102107 /usr/local/mysql (deleted)mysqld_sa 11317 root 255r REG 8,6 13620 102182 /usr/local/mysql/bin/mysqld_safe (deleted)mysqld 11353 mysql txt REG 8,6 29382763 102138 /usr/local/mysql/bin/mysqld (deleted)mysqld 11353 mysql 5u REG 8,9 0 58761 /tmp/ibpfbHsa (deleted)mysqld 11353 mysql 6u REG 8,9 0 58771 /tmp/ibqoMnag (deleted)mysqld 11353 mysql 7u REG 8,9 0 58772 /tmp/ibN4C4Rl (deleted)mysqld 11353 mysql 8u REG 8,9 0 58781 /tmp/ibkWP8zr (deleted)mysqld 11353 mysql 12u REG 8,9 0 58782 /tmp/ibnqrZsx (deleted)java 16961 root 1w REG 8,3 5822263296 457562 /opt/tomcat/logs/catalina.out (deleted)java 16961 root 2w REG 8,3 5822263296 457562 /opt/tomcat/logs/catalina.out (deleted)java 16961 root 12w REG 8,3 5469261824 197191 /opt/tomcat/logs/catalina.2013-01-02.log (deleted)java 16961 root 13w REG 8,3 2166784 197192 /opt/tomcat/logs/localhost.2013-01-02.log (deleted)java 16961 root 14w REG 8,3 0 391681 /opt/tomcat/logs/manager.2013-01-02.log (deleted)java 16961 root 15w REG 8,3 0 391682 /opt/tomcat/logs/host-manager.2013-01-02.log (deleted) 关闭删除文件进程1[root@localhost /]# kill -9 16961 磁盘容量恢复]]></content>
  </entry>
  <entry>
    <title><![CDATA[Keepalived+Nginx+Tomcat 实现高可用Web集群]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FKeepalived%2BNginx%2BTomcat_%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8Web%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[集群规划图片 一、Nginx的安装过程1.下载Nginx安装包，安装依赖环境包(1)安装 C++编译环境1yum -y install gcc #C++ (2)安装pcre1yum -y install pcre-devel (3)安装zlib1yum -y install zlib-devel (4)安装Nginx定位到nginx 解压文件位置，执行编译安装命令123[root@localhost nginx-1.12.2]# pwd/usr/local/nginx/nginx-1.12.2[root@localhost nginx-1.12.2]# ./configure &amp;&amp; make &amp;&amp; make install (5)启动Nginx安装完成后先寻找那安装完成的目录位置123[root@localhost nginx-1.12.2]# whereis nginxnginx: /usr/local/nginx[root@localhost nginx-1.12.2]# 进入Nginx子目录sbin启动Nginxnginx]# cd sbin/12345[root@localhost sbin]# lsnginx[root@localhost sbin]# ./nginx &amp;[1] 5768[root@localhost sbin]# 查看Nginx是否启动 Niginx启动成功截图 或通过进程查看Nginx启动情况123456[root@localhost sbin]# ps -aux|grep nginxroot 5769 0.0 0.0 20484 608 ? Ss 14:03 0:00 nginx: master process ./nginxnobody 5770 0.0 0.0 23012 1620 ? S 14:03 0:00 nginx: worker processroot 5796 0.0 0.0 112668 972 pts/0 R+ 14:07 0:00 grep --color=auto nginx[1]+ 完成 ./nginx[root@localhost sbin]# 到此Nginx安装完成并启动成功。 (6)Nginx快捷启动和开机启动配置编辑Nginx快捷启动脚本【注意Nginx安装路径，需要根据自己的NGINX路径进行改动】1[root@localhost init.d]# vim /etc/rc.d/init.d/nginx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15# description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /usr/local/nginx/conf/nginx.conf# pidfile: /usr/local/nginx/logs/nginx.pid # Source function library.. /etc/rc.d/init.d/functions # Source networking configuration.. /etc/sysconfig/network # Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx)NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot;[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginxlockfile=/var/lock/subsys/nginx make_dirs() &#123; # make required directories user=`$nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` if [ -z &quot;`grep $user /etc/passwd`&quot; ]; then useradd -M -s /bin/nologin $user fi options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125; start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125; stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125; restart() &#123; #configtest || return $? stop sleep 1 start&#125; reload() &#123; #configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125; force_reload() &#123; restart&#125; configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125; rh_status() &#123; status $prog&#125; rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125; case &quot;$1&quot; instart)rh_status_q &amp;&amp; exit 0$1;;stop) rh_status_q || exit 0$1;;restart|configtest)$1;;reload)rh_status_q || exit 7$1;;force-reload)force_reload;;status)rh_status;;condrestart|try-restart)rh_status_q || exit 0;;*)echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot;exit 2esac 为启动脚本授权 并加入开机启动12[root@localhost init.d]# chmod -R 777 /etc/rc.d/init.d/nginx [root@localhost init.d]# chkconfig nginx 启动Nginx1[root@localhost init.d]# ./nginx start 将Nginx加入系统环境变量1[root@localhost init.d]# echo &apos;export PATH=$PATH:/usr/local/nginx/sbin&apos;&gt;&gt;/etc/profile &amp;&amp; source /etc/profile Nginx命令 [ service nginx (start|stop|restart) ]12[root@localhost init.d]# service nginx startStarting nginx (via systemctl): [ 确定 ] Tips:快捷命令1service nginx (start|stop|restart) 二、KeepAlived安装和配置1.安装Keepalived依赖环境12345yum install -y popt-devel yum install -y ipvsadmyum install -y libnl*yum install -y libnf*yum install -y openssl-devel 2.编译Keepalived并安装12[root@localhost keepalived-1.3.9]# ./configure[root@localhost keepalived-1.3.9]# make &amp;&amp; make install 3.将Keepalive 安装成系统服务12[root@localhost etc]# mkdir /etc/keepalived[root@localhost etc]# cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ 手动复制默认的配置文件到默认路径123[root@localhost etc]# mkdir /etc/keepalived[root@localhost etc]# cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/[root@localhost etc]# cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ 为keepalived 创建软链接1[root@localhost sysconfig]# ln -s /usr/local/keepalived/sbin/keepalived /usr/sbin/ 设置Keepalived开机自启动123[root@localhost sysconfig]# chkconfig keepalived on注意：正在将请求转发到“systemctl enable keepalived.service”。Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service 启动Keepalived服务1[root@localhost keepalived]# keepalived -D -f /etc/keepalived/keepalived.conf 关闭Keepalived服务1[root@localhost keepalived]# killall keepalived 三、集群规划和搭建 集群规划图片 环境准备：CentOS 7.2 Keepalived &nbsp; Version 1.4.0 - December 29, 2017 Nginx &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Version: nginx/1.12.2 Tomcat &nbsp; &nbsp; &nbsp; &nbsp; Version:8集群规划清单虚拟机 | IP |说明—|—|——-Keepalived+Nginx1[Master] | 192.168.43.101 | Nginx Server 01Keeepalived+Nginx[Backup] | 192.168.43.102 | Nginx Server 02Tomcat01|192.168.43.103|Tomcat Web Server01Tomcat02|192.168.43.104|Tomcat Web Server02VIP|192.168.43.150|虚拟漂移IP 1.更改Tomcat默认欢迎页面，用于标识切换Web 更改TomcatServer01 节点ROOT/index.jsp 信息，加入TomcatIP地址，并加入Nginx值，即修改节点192.168.43.103信息如下：123&lt;div id=&quot;asf-box&quot;&gt; &lt;h1&gt;$&#123;pageContext.servletContext.serverInfo&#125;(192.168.224.103)&lt;%=request.getHeader(&quot;X-NGINX&quot;)%&gt;&lt;/h1&gt;&lt;/div&gt; 更改TomcatServer02 节点ROOT/index.jsp信息，加入TomcatIP地址，并加入Nginx值，即修改节点192.168.43.104信息如下：123&lt;div id=&quot;asf-box&quot;&gt; &lt;h1&gt;$&#123;pageContext.servletContext.serverInfo&#125;(192.168.224.104)&lt;%=request.getHeader(&quot;X-NGINX&quot;)%&gt;&lt;/h1&gt;&lt;/div&gt; 2.启动Tomcat服务，查看Tomcat服务IP信息，此时Nginx未启动，因此request-header没有Nginx信息。 Tomcat启动信息 3.配置Nginx代理信息1.配置Master节点[192.168.43.101]代理信息 1234567891011 upstream tomcat &#123; server 192.168.43.103:8080 weight=1; server 192.168.43.104:8080 weight=1;&#125;server&#123; location / &#123; proxy_pass http://tomcat; proxy_set_header X-NGINX &quot;NGINX-1&quot;; &#125; #......其他省略&#125; 2.配置Backup节点[192.168.43.102]代理信息1234567891011upstream tomcat &#123; server 192.168.43.103:8080 weight=1; server 192.168.43.104:8080 weight=1;&#125;server&#123; location / &#123; proxy_pass http://tomcat; proxy_set_header X-NGINX &quot;NGINX-2&quot;; &#125; #......其他省略&#125; 3.启动Master 节点Nginx服务12[root@localhost init.d]# service nginx startStarting nginx (via systemctl): [ 确定 ] 此时访问 192.168.43.101 可以看到103和104节点Tcomat交替显示，说明Nginx服务已经将请求负载到了2台tomcat上。 Nginx 负载效果 4.同理配置Backup[192.168.43.102] Nginx信息，启动Nginx后，访问192.168.43.102后可以看到Backup节点已起到负载的效果。 Backup负载效果 4.配置Keepalived 脚本信息1.在Master节点和Slave节点 /etc/keepalived目录下添加check_nginx.sh 文件，用于检测Nginx的存货状况，添加keepalived.conf文件 check_nginx.sh文件信息如下： 123456789101112131415#!/bin/bash#时间变量，用于记录日志d=`date --date today +%Y%m%d_%H:%M:%S`#计算nginx进程数量n=`ps -C nginx --no-heading|wc -l`#如果进程为0，则启动nginx，并且再次检测nginx进程数量，#如果还为0，说明nginx无法启动，此时需要关闭keepalivedif [ $n -eq &quot;0&quot; ]; then /etc/rc.d/init.d/nginx start n2=`ps -C nginx --no-heading|wc -l` if [ $n2 -eq &quot;0&quot; ]; then echo &quot;$d nginx down,keepalived will stop&quot; &gt;&gt; /var/log/check_ng.log systemctl stop keepalived fifi 添加完成后，为check_nginx.sh 文件授权，便于脚本获得执行权限。 1[root@localhost keepalived]# chmod -R 777 /etc/keepalived/check_nginx.sh 2.在Master 节点 /etc/keepalived目录下，添加keepalived.conf 文件，具体信息如下：123456789101112131415161718192021222324252627282930vrrp_script chk_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot; //检测nginx进程的脚本 interval 2 weight -20 &#125; global_defs &#123; notification_email &#123; //可以添加邮件提醒 &#125; &#125; vrrp_instance VI_1 &#123; state MASTER #标示状态为MASTER 备份机为BACKUP interface ens33 #设置实例绑定的网卡(ip addr查看，需要根据个人网卡绑定) virtual_router_id 51 #同一实例下virtual_router_id必须相同 mcast_src_ip 192.168.43.101 priority 250 #MASTER权重要高于BACKUP 比如BACKUP为240 advert_int 1 #MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 nopreempt #非抢占模式 authentication &#123; #设置认证 auth_type PASS #主从服务器验证方式 auth_pass 123456 &#125; track_script &#123; check_nginx &#125; virtual_ipaddress &#123; #设置vip 192.168.43.150 #可以多个虚拟IP，换行即可 &#125; &#125; 3.在Backup节点 etc/keepalived目录下添加 keepalived.conf 配置文件信息如下： 123456789101112131415161718192021222324252627282930vrrp_script chk_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot; //检测nginx进程的脚本 interval 2 weight -20 &#125; global_defs &#123; notification_email &#123; //可以添加邮件提醒 &#125; &#125; vrrp_instance VI_1 &#123; state BACKUP #标示状态为MASTER 备份机为BACKUP interface ens33 #设置实例绑定的网卡(ip addr查看) virtual_router_id 51 #同一实例下virtual_router_id必须相同 mcast_src_ip 192.168.43.102 priority 240 #MASTER权重要高于BACKUP 比如BACKUP为240 advert_int 1 #MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒 nopreempt #非抢占模式 authentication &#123; #设置认证 auth_type PASS #主从服务器验证方式 auth_pass 123456 &#125; track_script &#123; check_nginx &#125; virtual_ipaddress &#123; #设置vip 192.168.43.150 #可以多个虚拟IP，换行即可 &#125; &#125; Tips:关于配置信息的几点说明 state - 主服务器需配成MASTER，从服务器需配成BACKUP interface - 这个是网卡名，我使用的是VM12.0的版本，所以这里网卡名为ens33 mcast_src_ip - 配置各自的实际IP地址 priority - 主服务器的优先级必须比从服务器的高，这里主服务器配置成250，从服务器配置成240 virtual_ipaddress - 配置虚拟IP（192.168.43.150） authentication - auth_pass主从服务器必须一致，keepalived靠这个来通信 virtual_router_id - 主从服务器必须保持一致5.集群高可用(HA)验证 Step1 启动Master机器的Keepalived和 Nginx服务 12[root@localhost keepalived]# keepalived -D -f /etc/keepalived/keepalived.conf[root@localhost keepalived]# service nginx start 查看服务启动进程1234[root@localhost keepalived]# ps -aux|grep nginxroot 6390 0.0 0.0 20484 612 ? Ss 19:13 0:00 nginx: master process /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confnobody 6392 0.0 0.0 23008 1628 ? S 19:13 0:00 nginx: worker processroot 6978 0.0 0.0 112672 968 pts/0 S+ 20:08 0:00 grep --color=auto nginx 查看Keepalived启动进程 12345[root@localhost keepalived]# ps -aux|grep keepalivedroot 6402 0.0 0.0 45920 1016 ? Ss 19:13 0:00 keepalived -D -f /etc/keepalived/keepalived.confroot 6403 0.0 0.0 48044 1468 ? S 19:13 0:00 keepalived -D -f /etc/keepalived/keepalived.confroot 6404 0.0 0.0 50128 1780 ? S 19:13 0:00 keepalived -D -f /etc/keepalived/keepalived.confroot 7004 0.0 0.0 112672 976 pts/0 S+ 20:10 0:00 grep --color=auto keepalived 使用 ip add 查看虚拟IP绑定情况，如出现192.168.43.150 节点信息则绑定到Master节点 12345678910111213141516171819202122232425[root@localhost keepalived]# ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:91:bf:59 brd ff:ff:ff:ff:ff:ff inet 192.168.43.101/24 brd 192.168.43.255 scope global ens33 valid_lft forever preferred_lft forever inet 192.168.43.150/32 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::9abb:4544:f6db:8255/64 scope link valid_lft forever preferred_lft forever inet6 fe80::b0b3:d0ca:7382:2779/64 scope link tentative dadfailed valid_lft forever preferred_lft forever inet6 fe80::314f:5fe7:4e4b:64ed/64 scope link tentative dadfailed valid_lft forever preferred_lft forever3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN qlen 1000 link/ether 52:54:00:2b:74:aa brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 1000 link/ether 52:54:00:2b:74:aa brd ff:ff:ff:ff:ff:ff Step 2 启动Backup节点Nginx服务和Keepalived服务，查看服务启动情况，如Backup节点出现了虚拟IP，则Keepalvied配置文件有问题，此情况称为脑裂。 1234567891011121314151617181920[root@localhost keepalived]# clear[root@localhost keepalived]# ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:14:df:79 brd ff:ff:ff:ff:ff:ff inet 192.168.43.102/24 brd 192.168.43.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::314f:5fe7:4e4b:64ed/64 scope link valid_lft forever preferred_lft forever3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN qlen 1000 link/ether 52:54:00:2b:74:aa brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 1000 link/ether 52:54:00:2b:74:aa brd ff:ff:ff:ff:ff:ff Step 3 验证服务浏览并多次强制刷新地址： http://192.168.43.150 ,可以看到103和104多次交替显示，并显示Nginx-1,则表明 Master节点在进行web服务转发。 Step 4 关闭Master keepalived服务和Nginx服务，访问Web服务观察服务转移情况 12[root@localhost keepalived]# killall keepalived[root@localhost keepalived]# service nginx stop 此时强制刷新192.168.43.150发现 页面交替显示103和104并显示Nginx-2 ，VIP已转移到192.168.43.102上，已证明服务自动切换到备份节点上。 Step 5 启动Master Keepalived 服务和Nginx服务此时再次验证发现，VIP已被Master重新夺回，并页面交替显示 103和104，此时显示Nginx-1四、Keepalived抢占模式和非抢占模式keepalived的HA分为抢占模式和非抢占模式，抢占模式即MASTER从故障中恢复后，会将VIP从BACKUP节点中抢占过来。非抢占模式即MASTER恢复后不抢占BACKUP升级为MASTER后的VIP。 非抢占模式配置： 1&gt; 在vrrp_instance块下两个节点各增加了nopreempt指令，表示不争抢vip 2&gt; 节点的state都为BACKUP两个keepalived节点都启动后，默认都是BACKUP状态，双方在发送组播信息后，会根据优先级来选举一个MASTER出来。由于两者都配置了nopreempt，所以MASTER从故障中恢复后，不会抢占vip。这样会避免VIP切换可能造成的服务延迟。]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7.2 安装Hive和Hive学习使用札记]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FCentOS_7.2_%E5%AE%89%E8%A3%85Hive%E5%92%8CHive%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8%E6%9C%AD%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Hive由Facebook开源，是一个构建在Hadoop之上的数据仓库将结构化的数据映射成表支持类SQL查询，Hive中称为HQL无法实时更新，只支持向现有表中追加数据。 Hive原理图.png Hive常用文件格式类型TEXTFILE 默认文件格式，建表时用户需要显示指定分隔符 存储方式：行存储 SequenceFile 二进制键值对序列化文件格式 存储方式：行存储 列式存储格式 RCFILE/ORC 存储方式：列存储常用数据类型1.整数类型 SMALLINT、INT、BIGINT 空间占用分别是1字节、2字节、4字节、8字节2.浮点类型 DOUBLE 空间占用分别是32位和64位浮点数3. 布尔类型BOOLEAN 用于存储true和false4.字符串文本类型STRING 存储变长字符串，对类型长度没有限制5.时间戳类型TIMESTAMP 存储精度为纳秒的时间戳复杂数据类型1.ARRAY 存储相同类型的数据，可以通过下标获取数据 定义：ARRAY 查询：array[index]2.MAP 存储键值对数据，键或者值的类型必须相同，通过键获取值。 定义：MAP 查询：map[‘key’]3.STRUCT 可以存储多种不同的数据类型，一旦声明好结构，各字段的位置不能够改变。 定义：STRUCT 查询：struct.fieldname一、Hive的安装1.下载Hive安装包并解压 1[hadoop@hadoop01 apps]$ tar -zxvf apache-hive-1.2.2-bin.tar.gz 2.使用Root用户创建软链接1[root@hadoop01 apps]# ln -s /home/hadoop/apps/apache-hive-1.2.2-bin /usr/local/hive 3.为Hive指定用户组1[root@hadoop01 apps]# chown -R hadoop:hadoop /usr/local/hive 4. 添加Hive到系统环境变量并生效1[root@hadoop01 apps]# vim /etc/profile 添加环境变量内容为： 123export HIVE_HOME=/usr/local/hiveexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$&#123;ZOOKEEPER_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;HIVE_HOME&#125;/bin 生效环境变量 1[root@hadoop01 apps]# source /etc/profile 5.配置Hive的默认metastore修改Hive配置目录下的hive-site.xml配置文件，编辑内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.43.50:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;xxxx&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 说明：数据库连接地址为Mysql地址，且所配置用户具有外网访问数据库权限,ConnectionPassword配置成个人Mysql数据库用户密码 二、Hive的使用1.运行hive123[hadoop@hadoop01 ~]$ hiveLogging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-1.2.2-bin/lib/hive-common-1.2.2.jar!/hive-log4j.properties 2.查看数据库1234hive&gt; show databases;OKdefaultTime taken: 0.99 seconds, Fetched: 1 row(s) 3.创建用户表： user_info字段信息 字段名称 字段类型 用户id string 地域id string 年龄 int 职业 string 123456789create table user_info(user_id string,area_id string,age int,occupation string)row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos;stored as textfile; 4.查看表查看default库中的表，发现新建的user_info表在default库中 1234hive&gt; show tables;OKuser_infoTime taken: 0.04 seconds, Fetched: 1 row(s) 查看对应文件目录信息 123[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouseFound 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-14 19:48 /user/hive/warehouse/user_info 5.hive删除表删除user_info表，user_info表在hdfs的目录也会被同时删除 123456hive&gt; drop table user_info;OKTime taken: 0.935 secondshive&gt; show tables;OKTime taken: 0.041 seconds 查看文件目录位置 12[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouse[hadoop@hadoop01 root]$ 6.创建数据库，用于存储维度12345678hive&gt; create database rel;OKTime taken: 0.098 secondshive&gt; show databases;OKdefaultrelTime taken: 0.025 seconds, Fetched: 2 row(s) 查看对应文件目录信息： 123[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouseFound 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-14 19:55 /user/hive/warehouse/rel.db 7.创建内部管理表在数据库rel中创建学生信息表，字段信息：学号、姓名、年龄、地域。切换使用rel数据库： 123456789101112use rel;create table student_info(student_id string comment &apos;学号&apos;,name string comment &apos;姓名&apos;,age int comment &apos;年龄&apos;,origin string comment &apos;地域&apos;)comment &apos;学生信息表&apos;row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos;stored as textfile; 查看对应目录信息 123[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouse/rel.dbFound 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-14 19:59 /user/hive/warehouse/rel.db/student_info 8.使用load从本地加载数据到表使用load从本地加载数据到表student_info 12345hive&gt; load data local inpath &apos;/home/hadoop/apps/hive_test_data/student_info_data.txt&apos; into table student_info;Loading data to table rel.student_infoTable rel.student_info stats: [numFiles=1, totalSize=341]OKTime taken: 1.144 seconds 查看student_info 表信息和对应文件路径 1234567891011121314151617181920212223hive&gt; select * from student_info;OK1 xiaoming 20 112 xiaobai 21 313 zhangfei 22 444 likui 19 445 zhaoyun 21 136 zhangsan 20 117 lisi 19 118 wangwu 23 319 zhaofei 19 2110 zhangyan 20 2111 lihe 20 2212 caoyang 17 3213 lihao 19 3214 zhaoming 21 5015 zhouhong 18 5116 yangshuo 23 3317 xiaofei 24 1318 liman 23 1319 qianbao 20 1320 sunce 21 41Time taken: 0.767 seconds, Fetched: 20 row(s) 查看对应文件夹路径信息 123[hadoop@hadoop01 hive_test_data]$ hadoop fs -ls /user/hive/warehouse/rel.db/student_infoFound 1 items-rwxr-xr-x 3 hadoop supergroup 341 2018-01-14 20:09 /user/hive/warehouse/rel.db/student_info/student_info_data.txt 9.使用load从HDFS上加载数据到表student_info先删除原有数据文件 123[hadoop@hadoop01 hive_test_data]$ hadoop fs -rm -f /user/hive/warehouse/rel.db/student_info/student_info_data.txt18/01/14 20:15:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /user/hive/warehouse/rel.db/student_info/student_info_data.txt 将本地文件上传到HDFS根目录下 123456789[hadoop@hadoop01 hive_test_data]$ hadoop fs -put /home/hadoop/apps/hive_test_data/student_info_data.txt /[hadoop@hadoop01 hive_test_data]$ hadoop fs -ls /Found 6 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-14 16:23 /addatadrwxr-xr-x - hadoop supergroup 0 2017-12-23 20:20 /data-rw-r--r-- 3 hadoop supergroup 341 2018-01-14 20:16 /student_info_data.txtdrwxrwx--- - hadoop supergroup 0 2018-01-14 17:26 /tmpdrwxr-xr-x - hadoop supergroup 0 2018-01-14 19:48 /userdrwxr-xr-x - hadoop supergroup 0 2018-01-13 16:26 /wordcount 使用load将HDFS文件加载到student_info 表中 12345678910111213141516171819202122232425262728hive&gt; load data inpath &apos;/student_info_data.txt&apos; into table student_info;Loading data to table rel.student_infoTable rel.student_info stats: [numFiles=1, totalSize=341]OKTime taken: 0.602 secondshive&gt; select * from student_info;OK1 xiaoming 20 112 xiaobai 21 313 zhangfei 22 444 likui 19 445 zhaoyun 21 136 zhangsan 20 117 lisi 19 118 wangwu 23 319 zhaofei 19 2110 zhangyan 20 2111 lihe 20 2212 caoyang 17 3213 lihao 19 3214 zhaoming 21 5015 zhouhong 18 5116 yangshuo 23 3317 xiaofei 24 1318 liman 23 1319 qianbao 20 1320 sunce 21 41Time taken: 0.143 seconds, Fetched: 20 row(s) 采用覆盖重写方式加载文件到student_info 表中 原hdfs根目录下的student_info_data.txt已经被剪切到student_info表的hdfs路径下/user/hive/warehouse/rel.db/student_info12345hive&gt; load data inpath &apos;/student_info_data.txt&apos; overwrite into table student_info;Loading data to table rel.student_infoTable rel.student_info stats: [numFiles=1, numRows=0, totalSize=341, rawDataSize=0]OKTime taken: 0.41 seconds 10.Hive的数据类型 字段名 类型 注释 user_id string 用户ID salary int 工资 worked_citys array 工作过的城市 social_security map 社保缴费情况(养老，医保) wealfare struct 福利(吃饭补助(float),是否转正(boolean),商业保险(float) 创建员工表 12345678910111213141516171819hive&gt; create table rel.employee( &gt; user_id string, &gt; salary int, &gt; worked_citys array&lt;string&gt;, &gt; social_security map&lt;string,float&gt;, &gt; welfare struct&lt;meal_allowance:float,if_regular:boolean,commercial_insurance:float&gt; &gt; ) &gt; row format delimited fields terminated by &apos;\t&apos; &gt; collection items terminated by &apos;,&apos; &gt; map keys terminated by &apos;:&apos; &gt; lines terminated by &apos;\n&apos; &gt; stored as textfile;OKTime taken: 0.212 secondshive&gt; show tables;OKemployeestudent_infoTime taken: 0.057 seconds, Fetched: 2 row(s) 从本地加载数据到表employee 1234567891011hive&gt; load data local inpath &apos;/home/hadoop/apps/hive_test_data/employee_data.txt&apos; into table employee;Loading data to table rel.employeeTable rel.employee stats: [numFiles=1, totalSize=206]OKTime taken: 0.388 secondshive&gt; select * from employee;OKzhangsan 10800 [&quot;beijing&quot;,&quot;shanghai&quot;] &#123;&quot;养老&quot;:1000.0,&quot;医疗&quot;:600.0&#125; &#123;&quot;meal_allowance&quot;:2000.0,&quot;if_regular&quot;:true,&quot;commercial_insurance&quot;:500.0&#125;lisi 20000 [&quot;beijing&quot;,&quot;nanjing&quot;] &#123;&quot;养老&quot;:2000.0,&quot;医疗&quot;:1200.0&#125; &#123;&quot;meal_allowance&quot;:2000.0,&quot;if_regular&quot;:false,&quot;commercial_insurance&quot;:500.0&#125;wangwu 17000 [&quot;shanghai&quot;,&quot;nanjing&quot;] &#123;&quot;养老&quot;:1800.0,&quot;医疗&quot;:1100.0&#125; &#123;&quot;meal_allowance&quot;:2000.0,&quot;if_regular&quot;:true,&quot;commercial_insurance&quot;:500.0&#125;Time taken: 0.127 seconds, Fetched: 3 row(s) 查询已转正的员工编号，工资，工作过的第一个城市，社保养老缴费情况，福利餐补金额 1234567select user_id,salary,worked_citys[0],social_security[&apos;养老&apos;],welfare.meal_allowance from rel.employeewhere welfare.if_regular=true; 11.创建外部表 【常用】可以提前创建好hdfs路径hadoop mkdir -p /user/hive/warehouse/data/student_school_info如果没有提前创建好，在创建外部表的时候会根据指定路径自动创建|字段名|字段类型|字段注释||:—-:|:—-:|:—-:||student_id|string|学生ID||name|string|学生姓名||institute_id|string|学院ID||major_id|string|专业ID||school_year|string|入学年份| 123456789101112create external table rel.student_school_info(student_id string,name string,institute_id string,major_id string,school_year string)row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos;stored as textfilelocation &apos;/user/hive/warehouse/data/student_school_info&apos;; 查看对应文件目录 123[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouse/data/Found 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-15 14:08 /user/hive/warehouse/data/student_school_info 上传本地数据文件到hdfs 1[hadoop@hadoop01 root]$ hadoop fs -put /home/hadoop/apps/hive_test_data/student_school_info_external_data.txt /user/hive/warehouse/data/student_school_info/ 12.创建内部分区表 字段名称 类型 注释 studnet_id string 学号 name string 姓名 institute_id string 学院ID major_id string 专业ID 1234567891011create table student_school_info_partition(student_id string,name string,institute_id string,major_id string)partitioned by(school_year string) row format delimitedfields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos;stored as textfile; 使用insert into从student_school_info表将2017年入学的学籍信息导入到student_school_info_partition分区表中 1234insert into table student_school_info_partition partition(school_year=&apos;2017&apos;)select t1.student_id,t1.name,t1.institute_id,t1.major_idfrom student_school_info t1where t1.school_year=2017; 13.查看分区1234hive&gt; show partitions student_school_info_partition;OKschool_year=2017Time taken: 0.191 seconds, Fetched: 1 row(s) 查看hdfs路径 123[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouse/rel.db/student_school_info_partition/Found 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-16 13:20 /user/hive/warehouse/rel.db/student_school_info_partition/school_year=2017 查询student_school_info_partition 1234567891011hive&gt; select * from student_school_info_partition where school_year=&apos;2017&apos;;OK1 xiaoming information software 20172 xiaobai information computer 20173 zhangfei information computer 20174 likui information bigdata 20175 zhaoyun information bigdata 20176 zhangsan information software 20177 lisi information bigdata 20178 wangwu information computer 2017Time taken: 0.226 seconds, Fetched: 8 row(s) 14.删除分区1234hive&gt; alter table student_school_info_partition drop partition (school_year=&apos;2017&apos;);Dropped the partition school_year=2017OKTime taken: 0.71 seconds 15.使用动态分区添加数据12345set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict;insert overwrite table student_school_info_partition partition(school_year)select t1.student_id,t1.name,t1.institute_id,t1.major_id,t1.school_yearfrom student_school_info t1 查看分区 1234hive&gt; show partitions student_school_info_partition;OKschool_year=2017Time taken: 0.12 seconds, Fetched: 1 row(s) 查看hdfs路径 123[hadoop@hadoop01 root]$ hadoop fs -ls /user/hive/warehouse/rel.db/student_school_info_partition/Found 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-01-16 13:27 /user/hive/warehouse/rel.db/student_school_info_partition/school_year=2017 15.创建外部分区表【常用】123456789101112create external table rel.student_school_info_external_partition(student_id string,name string,institute_id string,major_id string)partitioned by(school_year string) row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos;stored as textfilelocation &apos;/user/hive/warehouse/data/student_school_info_external_partition&apos;; 在分区表的hdfs路径中添加school_year=2017目录 1hadoop fs -mkdir /user/hive/warehouse/data/student_school_info_external_partition/school_year=2017 将student_school_external_partition_data.txt文件上传到school_year=2017文件夹下 1hadoop fs -put student_school_external_partition_data.txt /user/hive/warehouse/data/student_school_info_external_partition/school_year=2017 虽然数据已经添加到了分区对应的hdfs路径，但是表还没有添加分区，所以查询的时候没有数据 手动添加分区 123hive&gt; alter table student_school_info_external_partition add partition(school_year=&apos;2017&apos;);OKTime taken: 0.111 seconds 1234567891011hive&gt; select * from student_school_info_external_partition;OK1 xiaoming information software 20172 xiaobai information computer 20173 zhangfei information computer 20174 likui information bigdata 20175 zhaoyun information bigdata 20176 zhangsan information software 20177 lisi information bigdata 20178 wangwu information computer 2017Time taken: 0.127 seconds, Fetched: 8 row(s) 删除分区 1234hive&gt; alter table student_school_info_external_partition drop partition(school_year=&apos;2017&apos;);Dropped the partition school_year=2017OKTime taken: 0.19 seconds 查看分区，分区已经被删除 123hive&gt; show partitions student_school_info_external_partition;OKTime taken: 0.168 seconds 查看hdfs分区数据，分区数据还在 123[hadoop@hadoop01 hive_test_data]$ hadoop fs -ls /user/hive/warehouse/data/student_school_info_external_partition/school_year=2017Found 1 items-rw-r--r-- 3 hadoop supergroup 250 2018-01-16 13:33 /user/hive/warehouse/data/student_school_info_external_partition/school_year=2017/student_school_external_partition_data.txt]]></content>
  </entry>
  <entry>
    <title><![CDATA[Centos7.2安装MariaDB]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FCentos7.2%E5%AE%89%E8%A3%85MariaDB%2F</url>
    <content type="text"><![CDATA[1.检查是否已经具有MariaDB相关安装，并删除已有安装12[root@hadoop01 home]# rpm -qa|grep mariadb #查询已安装包mariadb-libs-5.5.52-1.el7.x86_64 12[root@hadoop01 home]# rpm -e --nodeps mariadb-* # 移除已安装包错误：未安装软件包 mariadb-* 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@hadoop01 home]# yum remove mysql mysql-server mysql-libs compat-mysql51 # 删除Mysql服务已加载插件：fastestmirror, langpacks参数 mysql 没有匹配参数 mysql-server 没有匹配参数 compat-mysql51 没有匹配正在解决依赖关系--&gt; 正在检查事务---&gt; 软件包 mariadb-libs.x86_64.1.5.5.52-1.el7 将被 删除--&gt; 正在处理依赖关系 libmysqlclient.so.18()(64bit)，它被软件包 2:postfix-2.10.1-6.el7.x86_64 需要--&gt; 正在处理依赖关系 libmysqlclient.so.18(libmysqlclient_18)(64bit)，它被软件包 2:postfix-2.10.1-6.el7.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 postfix.x86_64.2.2.10.1-6.el7 将被 删除 &gt; 解决依赖关系完成▽ase/7/x86_64 | 3.6 kB 00:00:00 extras/7/x86_64 | 3.4 kB 00:00:00 updates/7/x86_64 | 3.4 kB 00:00:00 依赖关系解决============================================================================================================================================================================================================================================ Package 架构 版本 源 大小============================================================================================================================================================================================================================================正在删除: mariadb-libs x86_64 1:5.5.52-1.el7 @anaconda 4.4 M为依赖而移除: postfix x86_64 2:2.10.1-6.el7 @anaconda 12 M事务概要============================================================================================================================================================================================================================================移除 1 软件包 (+1 依赖软件包)安装大小：17 M是否继续？[y/N]：yDownloading packages:Running transaction checkRunning transaction testTransaction test succeededRunning transaction 正在删除 : 2:postfix-2.10.1-6.el7.x86_64 1/2 正在删除 : 1:mariadb-libs-5.5.52-1.el7.x86_64 2/2 验证中 : 2:postfix-2.10.1-6.el7.x86_64 1/2 验证中 : 1:mariadb-libs-5.5.52-1.el7.x86_64 2/2 删除: mariadb-libs.x86_64 1:5.5.52-1.el7 作为依赖被删除: postfix.x86_64 2:2.10.1-6.el7 完毕！ 2.增加MariaDB的仓库源12345678[root@hadoop01 home]#vi /etc/yum.repos.d/MariaDB.repo 增加MariaDB的数据库镜像信息# MariaDB 10.2 CentOS repository list - created 2017-12-26 06:46 UTC# http://downloads.mariadb.org/mariadb/repositories/[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.2/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 文件信息请参考[https://downloads.mariadb.org/mariadb/repositories/#mirror=tuna] 设置具体需要安装的版本，本文下载MariaDB10.2稳定版。 3.安装MariaDB123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@localhost ~]# yum -y install MariaDB-server MariaDB-client已加载插件：fastestmirror, langpacksmariadb | 2.9 kB 00:00:00 mariadb/primary_db | 21 kB 00:00:01 Loading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.sohu.com * updates: mirrors.aliyun.com正在解决依赖关系--&gt; 正在检查事务---&gt; 软件包 MariaDB-client.x86_64.0.10.2.11-1.el7.centos 将被 安装--&gt; 正在处理依赖关系 MariaDB-common，它被软件包 MariaDB-client-10.2.11-1.el7.centos.x86_64 需要---&gt; 软件包 MariaDB-server.x86_64.0.10.2.11-1.el7.centos 将被 安装--&gt; 正在处理依赖关系 perl(DBI)，它被软件包 MariaDB-server-10.2.11-1.el7.centos.x86_64 需要--&gt; 正在处理依赖关系 galera，它被软件包 MariaDB-server-10.2.11-1.el7.centos.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 MariaDB-common.x86_64.0.10.2.11-1.el7.centos 将被 安装--&gt; 正在处理依赖关系 MariaDB-compat，它被软件包 MariaDB-common-10.2.11-1.el7.centos.x86_64 需要---&gt; 软件包 galera.x86_64.0.25.3.22-1.rhel7.el7.centos 将被 安装--&gt; 正在处理依赖关系 libboost_program_options.so.1.53.0()(64bit)，它被软件包 galera-25.3.22-1.rhel7.el7.centos.x86_64 需要---&gt; 软件包 perl-DBI.x86_64.0.1.627-4.el7 将被 安装--&gt; 正在处理依赖关系 perl(RPC::PlServer) &gt;= 0.2001，它被软件包 perl-DBI-1.627-4.el7.x86_64 需要--&gt; 正在处理依赖关系 perl(RPC::PlClient) &gt;= 0.2000，它被软件包 perl-DBI-1.627-4.el7.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 MariaDB-compat.x86_64.0.10.2.11-1.el7.centos 将被 安装---&gt; 软件包 boost-program-options.x86_64.0.1.53.0-27.el7 将被 安装---&gt; 软件包 perl-PlRPC.noarch.0.0.2020-14.el7 将被 安装--&gt; 正在处理依赖关系 perl(Net::Daemon) &gt;= 0.13，它被软件包 perl-PlRPC-0.2020-14.el7.noarch 需要--&gt; 正在处理依赖关系 perl(Net::Daemon::Test)，它被软件包 perl-PlRPC-0.2020-14.el7.noarch 需要--&gt; 正在处理依赖关系 perl(Net::Daemon::Log)，它被软件包 perl-PlRPC-0.2020-14.el7.noarch 需要--&gt; 正在处理依赖关系 perl(Compress::Zlib)，它被软件包 perl-PlRPC-0.2020-14.el7.noarch 需要--&gt; 正在检查事务---&gt; 软件包 perl-IO-Compress.noarch.0.2.061-2.el7 将被 安装--&gt; 正在处理依赖关系 perl(Compress::Raw::Zlib) &gt;= 2.061，它被软件包 perl-IO-Compress-2.061-2.el7.noarch 需要--&gt; 正在处理依赖关系 perl(Compress::Raw::Bzip2) &gt;= 2.061，它被软件包 perl-IO-Compress-2.061-2.el7.noarch 需要---&gt; 软件包 perl-Net-Daemon.noarch.0.0.48-5.el7 将被 安装--&gt; 正在检查事务---&gt; 软件包 perl-Compress-Raw-Bzip2.x86_64.0.2.061-3.el7 将被 安装---&gt; 软件包 perl-Compress-Raw-Zlib.x86_64.1.2.061-4.el7 将被 安装--&gt; 解决依赖关系完成依赖关系解决============================================================================================================================================================================================================================================ Package 架构 版本 源 大小============================================================================================================================================================================================================================================正在安装: MariaDB-client x86_64 10.2.11-1.el7.centos mariadb 48 M MariaDB-server x86_64 10.2.11-1.el7.centos mariadb 110 M为依赖而安装: MariaDB-common x86_64 10.2.11-1.el7.centos mariadb 154 k MariaDB-compat x86_64 10.2.11-1.el7.centos mariadb 2.8 M boost-program-options x86_64 1.53.0-27.el7 base 156 k galera x86_64 25.3.22-1.rhel7.el7.centos mariadb 8.0 M perl-Compress-Raw-Bzip2 x86_64 2.061-3.el7 base 32 k perl-Compress-Raw-Zlib x86_64 1:2.061-4.el7 base 57 k perl-DBI x86_64 1.627-4.el7 base 802 k perl-IO-Compress noarch 2.061-2.el7 base 260 k perl-Net-Daemon noarch 0.48-5.el7 base 51 k perl-PlRPC noarch 0.2020-14.el7 base 36 k事务概要============================================================================================================================================================================================================================================ 等待下载完成,下载完成后会自动安装。 4.MariaDB服务管理12345[root@hadoop01 home]# systemctl start mariadb # 开启数据库服务[root@hadoop01 home]# systemctl enable mariadb # 开机自启动[root@hadoop01 home]# systemctl restart mariadb # 重启服务 [root@hadoop01 home]# systemctl status mariadb #查看数据库状态[root@hadoop01 home]# systemctl stop mariadb.service # 停止数据库服务 5.数据库登录12345678910[root@hadoop01 home]# mysql -urootWelcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 8Server version: 10.2.11-MariaDB MariaDB ServerCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.MariaDB [(none)]&gt; 6.设置数据库密码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@hadoop01 home]# mysql_secure_installation #初始化密码NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!In order to log into MariaDB to secure it, we&apos;ll need the currentpassword for the root user. If you&apos;ve just installed MariaDB, andyou haven&apos;t set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none): #输入当前密码，一般没设置直接回车ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)Enter current password for root (enter for none): OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MariaDBroot user without the proper authorisation.Set root password? [Y/n] y #是否设置root密码，输入yNew password: #输入新密码Re-enter new password: #重复密码 Password updated successfully!Reloading privilege tables.. ... Success!By default, a MariaDB installation has an anonymous user, allowing anyoneto log into MariaDB without having to have a user account created forthem. This is intended only for testing, and to make the installationgo a bit smoother. You should remove them before moving into aproduction environment.Remove anonymous users? [Y/n] #删除匿名用户 y ... Success!Normally, root should only be allowed to connect from &apos;localhost&apos;. Thisensures that someone cannot guess at the root password from the network.Disallow root login remotely? [Y/n] #禁止root远程登录 ... Success!By default, MariaDB comes with a database named &apos;test&apos; that anyone canaccess. This is also intended only for testing, and should be removedbefore moving into a production environment.Remove test database and access to it? [Y/n] #是否删除test数据库 - Dropping test database... ... Success! - Removing privileges on test database... ... Success!Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.Reload privilege tables now? [Y/n] #是否重新加载权限表 ... Success!Cleaning up...All done! If you&apos;ve completed all of the above steps, your MariaDBinstallation should now be secure.Thanks for using MariaDB! 测试登录1234567891011[root@hadoop01 home]# mysql -uroot -pEnter password: Welcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 22Server version: 10.2.11-MariaDB MariaDB ServerCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.MariaDB [(none)]&gt; 7.配置MariaDB数据库字符集1234[root@hadoop01 home]# cd /etc/my.cnf.d/[root@hadoop01 my.cnf.d]# lsenable_encryption.preset mysql-clients.cnf server.cnf[root@hadoop01 my.cnf.d]# vim server.cnf (1).在server.cnf 文件在[mysqld]标签下增加以下信息：12345init_connect=&apos;SET collation_connection = utf8_unicode_ci&apos; init_connect=&apos;SET NAMES utf8&apos; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake (2).在mysql-clients.cnf 文件[mysql]标签下增加如下信息：1default-character-set=utf8 全部配置完成后重启数据库服务1[root@hadoop01 my.cnf.d]# systemctl restart mariadb 之后进入MariaDB查看字符集123456789101112131415161718192021222324252627282930313233343536[root@hadoop01 my.cnf.d]# systemctl restart mariadb[root@hadoop01 my.cnf.d]# mysql -uroot -pEnter password: Welcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 9Server version: 10.2.11-MariaDB MariaDB ServerCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.MariaDB [(none)]&gt; show variables like &quot;%character%&quot;;show variables like &quot;%collation%&quot;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_unicode_ci || collation_database | utf8_unicode_ci || collation_server | utf8_unicode_ci |+----------------------+-----------------+3 rows in set (0.00 sec)MariaDB [(none)]&gt; 字符集配置完成 8.创建用户、添加授权(1).创建用户12MariaDB [(none)]&gt; create user hadoop@localhost identified by &apos;xxxx&apos;; #请替换xx为密码Query OK, 0 rows affected (0.01 sec) (2).为用户进行操作授权12MariaDB [(none)]&gt; grant all on *.* to hadoop@locahost identified by &apos;xxxx&apos;; #请替换xx为密码Query OK, 0 rows affected (0.00 sec) (3).授权外网登录权限12MariaDB [(none)]&gt; grant all privileges on *.* to hadoop@&apos;%&apos; identified by &apos;xxx&apos;; #请替换xx为密码Query OK, 0 rows affected (0.00 sec) 查询用户授权结果123456789101112MariaDB [mysql]&gt; select host,user,password from user;+-----------+--------+-------------------------------------------+| host | user | password |+-----------+--------+-------------------------------------------+| localhost | root | *7D8990305DAAE2A688433D400E6559EBDF439529 || 127.0.0.1 | root | *7D8990305DAAE2A688433D400E6559EBDF439529 || ::1 | root | *7D8990305DAAE2A688433D400E6559EBDF439529 || locahost | hadoop | *7D8990305DAAE2A688433D400E6559EBDF439529 || localhost | hadoop | *AB7E9F716159ED905A3E5DA78DA0DFD516C429E1 || % | hadoop | *7D8990305DAAE2A688433D400E6559EBDF439529 |+-----------+--------+-------------------------------------------+6 rows in set (0.00 sec)]]></content>
  </entry>
  <entry>
    <title><![CDATA[java 实现FastDFS文件操作]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2Fjava_%E5%AE%9E%E7%8E%B0FastDFS%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[java 实现FastDFS 文件操作1.下载FastClient并实现依赖Jar安装访问余大github项目 地址为fastdfs-client-java下载后导入Idea，修改项目Java编译版本，如图所示： fastDFS java项目截图 使用Maven进行 编译和构建，dos窗口定位到该项目路径下，进行编译和构建 1E:\fastdfs-client-java-master&gt;mvn clean install 构建成功后，会在maven 本地仓库出现相关Jar包，如图所示： 依赖Jar构建成功 在需要进行文件操作的项目模块增加Pom文件依赖配置，配置内容如下： 123456&lt;!-- fastdfs上传下载图片 路径和上面的pom中对应 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.csource.fastdfs-client-java&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client-java&lt;/artifactId&gt; &lt;version&gt;1.27&lt;/version&gt; &lt;/dependency&gt; 2.增加FastDFS连接配置文件在需要的项目模块资源配置文件夹下 src/resource 目录下新增配置文件 fdfs_client.properties配置内容具体如下： 1234567connect_timeout = 2network_timeout = 30charset = UTF-8http.tracker_http_port = 8088 # tracker Http端口http.anti_steal_token = no # 暂无作用http.secret_key = FastDFS1234567890 # 暂无作用tracker_server = 192.168.43.60:22122 # tracker Server地址信息 3.编写FastClient 工具类，用于封装文件操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211package com.gsww.ctyxy.util;import org.csource.common.MyException;import org.csource.common.NameValuePair;import org.csource.fastdfs.*;import java.io.BufferedOutputStream;import java.io.IOException;import java.net.URLDecoder;/** * FastDFS工具类【实现文件上传、下载、删除、查询】 * @author Zhangyongliang */public class FastDFSClient &#123; private TrackerClient trackerClient = null; private TrackerServer trackerServer = null; private StorageServer storageServer = null; private StorageClient1 storageClient = null; public FastDFSClient(String conf) throws Exception &#123; if (conf.contains(&quot;classpath:&quot;)) &#123; String path = URLDecoder.decode(getClass().getProtectionDomain().getCodeSource().getLocation().toString(),&quot;UTF-8&quot;); path=path.substring(6); conf = conf.replace(&quot;classpath:&quot;,URLDecoder.decode(path,&quot;UTF-8&quot;)); &#125; ClientGlobal.init(conf); trackerClient = new TrackerClient(); trackerServer = trackerClient.getConnection(); storageServer = null; storageClient = new StorageClient1(trackerServer, storageServer); &#125; /** * 上传文件方法 * &lt;p&gt;Title: uploadFile&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @param fileName 文件全路径 * @param extName 文件扩展名，不包含（.） * @param metas 文件扩展信息 * @return * @throws Exception */ public String uploadFile(String fileName, String extName, NameValuePair[] metas) &#123; String result=null; try &#123; result = storageClient.upload_file1(fileName, extName, metas); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (MyException e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 上传文件,传fileName * @param fileName 文件的磁盘路径名称 如：D:/image/aaa.jpg * @return null为失败 */ public String uploadFile(String fileName) &#123; return uploadFile(fileName, null, null); &#125; /** * * @param fileName 文件的磁盘路径名称 如：D:/image/aaa.jpg * @param extName 文件的扩展名 如 txt jpg等 * @return null为失败 */ public String uploadFile(String fileName, String extName) &#123; return uploadFile(fileName, extName, null); &#125; /** * 上传文件方法 * &lt;p&gt;Title: uploadFile&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @param fileContent 文件的内容，字节数组 * @param extName 文件扩展名 * @param metas 文件扩展信息 * @return * @throws Exception */ public String uploadFile(byte[] fileContent, String extName, NameValuePair[] metas) &#123; String result=null; try &#123; result = storageClient.upload_file1(fileContent, extName, metas); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (MyException e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 上传文件 * @param fileContent 文件的字节数组 * @return null为失败 * @throws Exception */ public String uploadFile(byte[] fileContent) throws Exception &#123; return uploadFile(fileContent, null, null); &#125; /** * 上传文件 * @param fileContent 文件的字节数组 * @param extName 文件的扩展名 如 txt jpg png 等 * @return null为失败 */ public String uploadFile(byte[] fileContent, String extName) &#123; return uploadFile(fileContent, extName, null); &#125; /** * 文件下载到磁盘 * @param path 图片路径 * @param output 输出流 中包含要输出到磁盘的路径 * @return -1失败,0成功 */ public int download_file(String path,BufferedOutputStream output) &#123; int result=-1; try &#123; byte[] b = storageClient.download_file1(path); try&#123; if(b != null)&#123; output.write(b); result=0; &#125; &#125;catch (Exception e)&#123;&#125; //用户可能取消了下载 finally &#123; if (output != null)&#123; try &#123; output.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 获取文件数组 * @param path 文件的路径 如group1/M00/00/00/wKgRsVjtwpSAXGwkAAAweEAzRjw471.jpg * @return */ public byte[] download_bytes(String path) &#123; byte[] b=null; try &#123; b = storageClient.download_file1(path); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (MyException e) &#123; e.printStackTrace(); &#125; return b; &#125; /** * 删除文件 * @param group 组名 如：group1 * @param storagePath 不带组名的路径名称 如：M00/00/00/wKgRsVjtwpSAXGwkAAAweEAzRjw471.jpg * @return -1失败,0成功 */ public Integer delete_file(String group ,String storagePath)&#123; int result=-1; try &#123; result = storageClient.delete_file(group, storagePath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (MyException e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * * @param storagePath 文件的全部路径 如：group1/M00/00/00/wKgRsVjtwpSAXGwkAAAweEAzRjw471.jpg * @return -1失败,0成功 * @throws IOException * @throws Exception */ public Integer delete_file(String storagePath)&#123; int result=-1; try &#123; result = storageClient.delete_file1(storagePath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (MyException e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 获取远程服务器文件资源信息 * @param groupName 文件组名 如：group1 * @param remoteFileName M00/00/00/wKgRsVjtwpSAXGwkAAAweEAzRjw471.jpg * @return */ public FileInfo getFile(String groupName,String remoteFileName)&#123; try &#123; return storageClient.get_file_info(groupName, remoteFileName); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 4.文件操作在web 项目Controller层进行文件的操作 上传文件12345678910@RequestMapping(value = &quot;file/uploadFast&quot;,method = RequestMethod.GET)public void uploadFast(HttpServletRequest request)throws Exception&#123; // 1、把FastDFS提供的jar包添加到工程中 // 2、初始化全局配置。加载一个配置文件。 String confUrl=this.getClass().getClassLoader().getResource(&quot;/fdfs_client.properties&quot;).getPath(); FastDFSClient fastDFSClient=new FastDFSClient(confUrl); //上传文件 String filePath= fastDFSClient.uploadFile(&quot;F:\\Photos\\P70602-192547.jpg&quot;); System.out.println(&quot;返回路径：&quot;+filePath); //省略其他 删除文件 123//删除文件 int flag=fastDFSClient.delete_file(&quot;group1/M00/00/00/wKgrPFpf94KASn3vAAsC7gailiI018.jpg&quot;); System.out.println(&quot;删除结果：&quot; +(flag==0?&quot;删除成功&quot;:&quot;删除失败&quot;)); 下载文件到桌面 12345//下载文件到用户桌面位置 FileSystemView fsv = FileSystemView.getFileSystemView(); File com=fsv.getHomeDirectory(); //读取桌面路径 int downFlag=fastDFSClient.download_file(&quot;group1/M00/00/00/wKgrPFpe9OqAWsHxAAH5yvc2jn8251.jpg&quot;,new BufferedOutputStream(new FileOutputStream(com.getPath()+&quot;\\aa.jpg&quot;))); System.out.println(&quot;下载结果为：&quot; +(downFlag==0?&quot;下载文件成功&quot;:&quot;下载文件失败&quot;)); 查询文件信息 123//获取文件信息 FileInfo file=fastDFSClient.getFile(&quot;group1&quot;,&quot;M00/00/00/wKgrPFpe9OqAWsHxAAH5yvc2jn8251.jpg&quot;); System.out.println(&quot;获取文件信息成功：&quot;+file.getFileSize()); 说明：在FastDFS工具类中集合了支持字节数组的上传入参。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群的安装部署和实践应用]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FKafka%E9%9B%86%E7%BE%A4%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%92%8C%E5%AE%9E%E8%B7%B5%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Kafka介绍Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。消息队列的作用 应用程序解耦并行处理 顺序保证 高吞吐率 高容错、高可用 可扩展 峰值处理 Kafka集群.png kafka原理Kafka集群由多个实例组成，每个节点称为Broker，对消息保存时根据Topic进行归类一个Topic可以被划分为多个Partition每个Partition可以有多个副本。 Kafka原理图01.png Partition内顺序存储，写入新消息采用追加的方式，消费消息采用FIFO的方式顺序拉取消息一个Topic可以有多个分区，Kafka只保证同一个分区内有序，不保证Topic整体（多个分区之间）有序 kafka原理图02.png Consumer Group（CG），为了加快读取速度，多个consumer可以划分为一个组，并行消费一个Toic,一个Topic可以由多个CG订阅，多个CG之间是平等的，同一个CG内可以有一个或多个consumer，同一个CG内的consumer之间是竞争 关系，一个消息在一个CG内的只能被一个consumer消费 kafka原理图03.png 一、Kafka集群部署集群规划清单 名称 节点 说明 节点名 Broker01 192.168.43.22 kafka节点01 hadoop03 Broker02 192.168.43.23 kafka节点02 hadoop04 Broker03 192.168.43.24 kafka节点03 hadoop05 Zookeeper 192.168.43.20/21/22 Zookeeper集群节点 hadoop01/hadoop02/hadoop03 1.下载Kafka安装包,并解压安装12345678910[root@hadoop03 kafka_2.11-0.10.2.1]# ll总用量 52drwxr-xr-x. 3 hadoop hadoop 4096 4月 22 2017 bindrwxr-xr-x. 2 hadoop hadoop 4096 4月 22 2017 configdrwxr-xr-x. 2 root root 152 1月 20 18:57 kafka-logsdrwxr-xr-x. 2 hadoop hadoop 4096 1月 20 18:43 libs-rw-r--r--. 1 hadoop hadoop 28824 4月 22 2017 LICENSEdrwxr-xr-x. 2 root root 4096 1月 20 23:07 logs-rw-r--r--. 1 hadoop hadoop 336 4月 22 2017 NOTICEdrwxr-xr-x. 2 hadoop hadoop 47 4月 22 2017 site-docs 2.创建软链接1[root@hadoop03 kafka_2.11-0.10.2.1]# ln -s /home/hadoop/apps/kafka_2.11-0.10.2.1 /usr/local/kafka 3.创建日志文件夹123[root@hadoop03 kafka]# pwd/usr/local/kafka[root@hadoop03 kafka]# mkdir kafka-logs/ 4.配置服务启动信息在/usr/local/kafka/config目录下修改server.properties文件，具体内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# see kafka.server.KafkaConfig for additional details and defaults############################# Server Basics ##############################每个borker的id是唯一的，多个broker要设置不同的idbroker.id=0#访问端口号port=9092#访问地址host.name=192.168.43.22#允许删除topicdelete.topic.enable=true# The number of threads handling network requestsnum.network.threads=3# The number of threads doing disk I/Onum.io.threads=8# The send buffer (SO_SNDBUF) used by the socket serversocket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket serversocket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)socket.request.max.bytes=104857600############################# Log Basics ##############################存储数据路径，默认是在/tmp目录下，需要修改log.dirs=/usr/local/kafka/kafka-logs#创建topic默认分区数num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.# This value is recommended to be increased for installations with data dirs located in RAID array.num.recovery.threads.per.data.dir=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync# the OS cache lazily. The following configurations control the flush of data to disk.# There are a few important trade-offs here:# 1. Durability: Unflushed data may be lost if you are not using replication.# 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.# 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.# The settings below allow one to configure the flush policy to flush data after a period of time or# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can# be set to delete segments after a period of time, or after a given size has accumulated.# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens# from the end of the log.#数据保存时间，默认7天，单位小时log.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining# segments don&apos;t drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000############################# Zookeeper ##############################zookeeper地址，多个地址用逗号隔开zookeeper.connect=192.168.43.20:2181,192.168.43.21:2181,192.168.43.22:2181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=6000 5.拷贝文件信息到Broker02/Broker03节点上12scp -r /home/hadoop/apps/kafka_2.11-0.10.2.1 hadoop@node04:/home/hadoop/apps/scp -r /home/hadoop/apps/kafka_2.11-0.10.2.1 hadoop@node04:/home/hadoop/apps/ 6.修改Broker02和Broker03信息创建软连接 1[root@hadoop03 kafka_2.11-0.10.2.1]# ln -s /home/hadoop/apps/kafka_2.11-0.10.2.1 /usr/local/kafka 修改配置文件server.properties信息12broker.id=1host.name=192.168.43.23 修改Broker03节点server.properties信息 12broker.id=2host.name=192.168.43.24 7.分别启动Broker01/Broker02/Broker03以后台进程的方式启动Kafka1[root@hadoop03 bin]#./kafka-server-start.sh -daemon config/server.properties 二、Kafka应用实践1.创建主题1234[root@hadoop03 bin]# pwd/usr/local/kafka/bin[root@hadoop03 bin]# ./kafka-topics.sh --create --zookeeper 192.168.43.20:2181 --replication-factor 2 --partitions 3 --topic topicnewtest1Created topic &quot;topicnewtest1&quot;. 2.查看主题12[root@hadoop03 bin]# ./kafka-topics.sh --list --zookeeper 192.168.43.20:2181topicnewtest1 3.查看主题信息12345[root@hadoop03 bin]# ./kafka-topics.sh --describe --zookeeper 192.168.43.20:2181 --topic topicnewtest1Topic:topicnewtest1 PartitionCount:3 ReplicationFactor:2 Configs: Topic: topicnewtest1 Partition: 0 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: topicnewtest1 Partition: 1 Leader: 0 Replicas: 0,1 Isr: 0,1 Topic: topicnewtest1 Partition: 2 Leader: 1 Replicas: 1,2 Isr: 1,2 4.删除主题123[root@hadoop03 bin]# ./kafka-topics.sh --delete --zookeeper 192.168.43.20:2181 --topic topicnewtest1Topic topicnewtest1 is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. 5.增加分区12345678910[root@hadoop03 bin]# ./kafka-topics.sh --alter --zookeeper 192.168.43.20:2181 --topic topicnewtest1 --partitions 5WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affectedAdding partitions succeeded![root@hadoop03 bin]# ./kafka-topics.sh --describe --zookeeper 192.168.43.20:2181 --topic topicnewtest1Topic:topicnewtest1 PartitionCount:5 ReplicationFactor:2 Configs: Topic: topicnewtest1 Partition: 0 Leader: 1 Replicas: 1,0 Isr: 1,0 Topic: topicnewtest1 Partition: 1 Leader: 2 Replicas: 2,1 Isr: 2,1 Topic: topicnewtest1 Partition: 2 Leader: 0 Replicas: 0,2 Isr: 0,2 Topic: topicnewtest1 Partition: 3 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: topicnewtest1 Partition: 4 Leader: 2 Replicas: 2,0 Isr: 2,0 6.使用kafka自带的生产者客户端脚本和消费端脚本使用kafka自带的生产者客户端脚本 1[root@hadoop03 bin]# ./kafka-console-producer.sh --broker-list 192.168.43.22:9092,192.168.43.23:9092 --topic topicnewtest1 使用kafka自带的消费者客户端脚本 1[root@hadoop04 bin]# ./kafka-console-consumer.sh --zookeeper 192.168.43.20:2181 --from-beginning --topic topicnewtest1 在生成端发送消息，可以在消费看到消息 7.使用Java访问Kafka产生消息和消费消息 Producer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package cn.chinahadoop.client;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Date;import java.util.Properties;import java.util.Random;/** * Kafka生产端 * @author Zhangyongliang */public class ProducerClient &#123; public static void main(String[] args)&#123; Properties props = new Properties(); //kafka broker列表 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.43.22:9092,192.168.43.23:9092,192.168.43.24:9092&quot;); //acks=1表示Broker接收到消息成功写入本地log文件后向Producer返回成功接收的信号，不需要等待所有的Follower全部同步完消息后再做回应 props.put(&quot;acks&quot;, &quot;1&quot;); //key和value的字符串序列化类 props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); //用户产生随机数，模拟消息生成 Random rand = new Random(); for(int i = 0; i &lt; 20; i++) &#123; //通过随机数产生一个ip地址作为key发送出去 String ip = &quot;192.168.1.&quot; + rand.nextInt(255); long runtime = new Date().getTime(); //组装一条消息内容 String msg = runtime + &quot;---&quot; + ip; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;send to kafka-&gt;key:&quot; + ip + &quot; value:&quot; + msg); //向kafka topictest1主题发送消息 producer.send(new ProducerRecord&lt;String, String&gt;(&quot;topicnewtest1&quot;, ip, msg)); &#125; producer.close(); &#125;&#125; ConSumer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package com.yongliang.kafka;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.Properties;/** * Kafka消费端 * @author Zhangyongliang */public class ConsumerClient &#123; /** * 手动提交偏移量 */ public static void manualCommintClient()&#123; Properties props = new Properties(); //kafka broker列表 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.43.22:9092,192.168.43.23:9092,192.168.43.24:9092&quot;); //consumer group id props.put(&quot;group.id&quot;, &quot;yongliang&quot;); //手动提交offset props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); //earliest表示从最早的偏移量开始拉取，latest表示从最新的偏移量开始拉取，none表示如果没有发现该Consumer组之前拉取的偏移量则抛异常。默认值latest。 props.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;); //key和value的字符串反序列化类 props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //consumer订阅topictest1主题，同时消费多个主题用逗号隔开 consumer.subscribe(Arrays.asList(&quot;topicnewtest1&quot;)); //每次最少处理10条消息后才提交 final int minBatchSize = 10; //用于保存消息的list List&lt;ConsumerRecord&lt;String, String&gt;&gt; bufferList = new ArrayList&lt;ConsumerRecord&lt;String, String&gt;&gt;(); while (true) &#123; System.out.println(&quot;--------------start pull message---------------&quot; ); long starttime = System.currentTimeMillis(); //poll方法需要传入一个超时时间，当没有可以拉取的消息时先等待， //如果已到超时时间还没有可以拉取的消息则进行下一轮拉取，单位毫秒 ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); long endtime = System.currentTimeMillis(); long tm = (endtime - starttime) / 1000; System.out.println(&quot;--------------end pull message and times=&quot; + tm + &quot;s -------------&quot;); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(&quot;partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.partition(), record.offset(), record.key(), record.value()); bufferList.add(record); &#125; System.out.println(&quot;--------------buffer size-&gt;&quot; + bufferList.size()); //如果读取到的消息满了10条, 就进行处理 if (bufferList.size() &gt;= minBatchSize) &#123; System.out.println(&quot;******start deal message******&quot;); try &#123; //当前线程睡眠1秒钟，模拟消息处理过程 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;manual commint offset start...&quot;); //处理完之后进行提交 consumer.commitSync(); //清除list, 继续接收 bufferList.clear(); System.out.println(&quot;manual commint offset end...&quot;); &#125; &#125; &#125; /** * 自动提交偏移量 */ public static void autoCommintClient()&#123; Properties props = new Properties(); //kafka broker列表 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.43.22:9092,192.168.43.23:9092,192.168.43.24:9092&quot;); props.put(&quot;group.id&quot;, &quot;newConsumerGroup&quot;); //自动提交 props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); //自动提交时间间隔1000毫秒 props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); //earliest表示从最早的偏移量开始拉取，latest表示从最新的偏移量开始拉取，none表示如果没有发现该Consumer组之前拉取的偏移量则抛异常。默认值latest。 props.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;); //key和value的字符串反序列化类 props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //consumer订阅topictest1主题，同时消费多个主题用逗号隔开 consumer.subscribe(Arrays.asList(&quot;topicnewtest1&quot;)); while (true) &#123; //poll方法需要传入一个超时时间，当没有可以拉取的消息时先等待， //如果已到超时时间还没有可以拉取的消息则进行下一轮拉取，单位毫秒 ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); //处理拉取过来的消息 for (ConsumerRecord&lt;String, String&gt; record : records)&#123; System.out.printf(&quot;partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.partition(), record.offset(), record.key(), record.value()); &#125; &#125; &#125; public static void main(String[] args)&#123; //自动提交offset// autoCommintClient(); //手动提交offset manualCommintClient(); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS 单机搭建FastDFS文件系统]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FCentOS_%E5%8D%95%E6%9C%BA%E6%90%AD%E5%BB%BAFastDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[此例为在CentOS7.2 单机上搭建一个FastDFS 文件管理系统 FastDFS架构图 安装所需文件均上传到百度云盘，位置：FastDFS百度云盘安装清单如下：|软件名称|版本|百度云盘存放名称||:—-:|:—-:|:—–:||FastDFS|5.11|fastdfs-5.11.zip||FastDFS-Nginx-module|无|fastdfs-nginx-module-master.zip||LibFastCommon|1.0.36|libfastcommon-1.0.36.zip||nginx|1.10.3|nginx-1.10.3.tar.gz| 一、安装FastDFS1.安装libfastcommon先解压安装包到目录 1[root@localhost fastDFS]# unzip libfastcommon-1.0.36.zip 解压后目录如下：1234567891011[root@localhost fastdfs-5.11]# ll[root@localhost libfastcommon-1.0.36]# ll总用量 32drwxr-xr-x. 2 root root 117 4月 5 2017 doc-rw-r--r--. 1 root root 8005 4月 5 2017 HISTORY-rw-r--r--. 1 root root 566 4月 5 2017 INSTALL-rw-r--r--. 1 root root 1606 4月 5 2017 libfastcommon.spec-rwxr-xr-x. 1 root root 3099 4月 5 2017 make.shdrwxr-xr-x. 2 root root 191 4月 5 2017 php-fastcommon-rw-r--r--. 1 root root 2763 4月 5 2017 READMEdrwxr-xr-x. 3 root root 4096 1月 17 11:21 src 安装C编译工具 gcc 1[root@localhost fastdfs-5.11]# yum -y install gcc-c++ 编译libfastcommon软件并安装 1[root@localhost fastdfs-5.11]# ./make.sh &amp;&amp; ./make.sh install 为libcommon 创建软链接到/usr/local/lib目录下 1234[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so 2.安装FastDFS解压安装包 1[root@localhost fastDFS]# unzip fastdfs-5.11.zip 解压后目录如下：1234567891011121314151617总用量 128drwxr-xr-x. 3 root root 4096 1月 17 11:25 clientdrwxr-xr-x. 2 root root 4096 1月 17 11:25 commondrwxr-xr-x. 2 root root 146 6月 3 2017 conf-rw-r--r--. 1 root root 35067 6月 3 2017 COPYING-3_0.txt-rw-r--r--. 1 root root 3171 6月 3 2017 fastdfs.spec-rw-r--r--. 1 root root 33100 6月 3 2017 HISTORYdrwxr-xr-x. 2 root root 48 6月 3 2017 init.d-rw-r--r--. 1 root root 7755 6月 3 2017 INSTALL-rwxr-xr-x. 1 root root 5548 6月 3 2017 make.shdrwxr-xr-x. 2 root root 4096 6月 3 2017 php_client-rw-r--r--. 1 root root 2380 6月 3 2017 README.md-rwxr-xr-x. 1 root root 1768 6月 3 2017 restart.sh-rwxr-xr-x. 1 root root 1680 6月 3 2017 stop.shdrwxr-xr-x. 4 root root 4096 1月 17 11:25 storagedrwxr-xr-x. 2 root root 4096 6月 3 2017 testdrwxr-xr-x. 2 root root 4096 1月 17 11:25 tracker 进入解压目录并进行编译和安装 12[root@localhost fastDFS]# cd fastdfs-5.11/[root@localhost fastdfs-5.11]# ./make.sh &amp;&amp; ./make.sh install 安装成功后，FastDFS会安装在/etc/fdfs目录下： 123456789101112[root@localhost fastdfs-5.11]# ll /etc/fdfs/总用量 76-rw-r--r--. 1 root root 316 1月 17 11:47 client.conf-rw-r--r--. 1 root root 1461 1月 17 11:25 client.conf.sample-rw-r--r--. 1 root root 955 1月 17 13:20 http.conf-rw-r--r--. 1 root root 31172 1月 17 13:21 mime.types-rw-r--r--. 1 root root 3716 1月 17 12:57 mod_fastdfs.conf-rw-r--r--. 1 root root 1278 1月 17 11:40 storage.conf-rw-r--r--. 1 root root 7927 1月 17 11:25 storage.conf.sample-rw-r--r--. 1 root root 105 1月 17 11:25 storage_ids.conf.sample-rw-r--r--. 1 root root 1356 1月 17 11:34 tracker.conf-rw-r--r--. 1 root root 7389 1月 17 11:25 tracker.conf.sample 我们需要把这三个示例文件复制一份，去掉.sample 123[root@localhost fdfs]# cp client.conf.sample client.conf[root@localhost fdfs]# cp storage.conf.sample storage.conf[root@localhost fdfs]# cp tracker.conf.sample tracker.conf FastDFS安装结束 二、安装Tracker1.创建tracker工作目录此目录用于保存tracker 的data和log 1[root@localhost fdfs]# mkdir /opt/fastdfs_tracker 2.配置tracker配置 /etc/fdfs目录下tracker.conf主要实现以下4个配置内容： 123451.disabled=false 2.port=22122 #默认端口号 3.base_path=/opt/fastdfs_tracker #我刚刚创建的目录 4.http.server_port=8080 #默认端口是80805.bind_addr= 0.0.0.0 监听地址 完整tracker.conf 文件信息如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103disabled=falsebind_addr= 0.0.0.0port=22122connect_timeout=30network_timeout=60base_path=/opt/fastdfs_trackermax_connections=512accept_threads=1work_threads=4min_buff_size = 8KBmax_buff_size = 128KBstore_lookup=2store_group=group2store_server=0store_path=0download_server=0reserved_storage_space = 10%log_level=inforun_by_group=run_by_user=allow_hosts=*sync_log_buff_interval = 10check_active_interval = 120thread_stack_size = 64KBstorage_ip_changed_auto_adjust = truestorage_sync_file_max_delay = 86400storage_sync_file_max_time = 300use_trunk_file = false slot_min_size = 256slot_max_size = 16MBtrunk_file_size = 64MBtrunk_create_file_advance = falsetrunk_create_file_time_base = 02:00trunk_create_file_interval = 86400trunk_create_file_space_threshold = 20Gtrunk_init_check_occupying = falsetrunk_init_reload_from_binlog = falsetrunk_compress_binlog_min_interval = 0use_storage_id = falsestorage_ids_filename = storage_ids.confid_type_in_filename = ipstore_slave_file_use_link = falserotate_error_log = falseerror_log_rotate_time=00:00rotate_error_log_size = 0log_file_keep_days = 0use_connection_pool = falseconnection_pool_max_idle_time = 3600http.server_port=8080http.check_alive_interval=30http.check_alive_type=tcphttp.check_alive_uri=/status.html 修改保存后创建软引用 1[root@localhost fdfs]# ln -s /usr/bin/fdfs_storaged /usr/local/bin 4.启动tracker，并加入开机启动项1[root@localhost fdfs]# service fdfs_trackerd start 进行刚刚创建的tracker目录，发现目录中多了data和log两个目录 1234[root@localhost fdfs]# ll /opt/fastdfs_tracker/总用量 0drwxr-xr-x. 2 root root 178 1月 17 13:48 datadrwxr-xr-x. 2 root root 26 1月 17 11:35 logs 将tracker加入开机启动项 1[root@localhost fdfs]# echo &quot;service fdfs_trackerd start&quot; |tee -a /etc/rc.d/rc.local 查看一下tracker的端口监听情况 12[root@localhost fdfs]# netstat -unltp|grep fdfstcp 0 0 0.0.0.0:22122 0.0.0.0:* LISTEN 3088/fdfs_trackerd 三、安装Storage1.配置storage工作目录由于storage还需要一个目录用来存储数据，因此多建了两个目录fastdfs_storage_data,fastdfs_storage1234567[root@localhost opt]# mkdir fastdfs_storage[root@localhost opt]# mkdir fastdfs_storage_data[root@localhost opt]# ll总用量 0drwxr-xr-x. 4 root root 30 1月 17 11:45 fastdfs_storagedrwxr-xr-x. 3 root root 18 1月 17 11:45 fastdfs_storage_datadrwxr-xr-x. 4 root root 30 1月 17 11:35 fastdfs_tracker 2.配置storage文件修改 /etc/fdfs 目录下 storage.conf 文件修改要点如下： 123456781.disabled=false 2.group_name=group1 #组名，根据实际情况修改 3.port=23000 #设置storage的端口号，默认是23000，同一个组的storage端口号必须一致 4.base_path=/opt/fastdfs_storage #设置storage数据文件和日志目录 5.store_path_count=1 #存储路径个数，需要和store_path个数匹配 6.store_path0=/opt/fastdfs_storage_data #实际文件存储路径 7.tracker_server=192.168.43.60:22122 #我CentOS7的ip地址 8.http.server_port=8888 #设置 http 端口号 完整信息如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556disabled=falsegroup_name=group1bind_addr= 0.0.0.0client_bind=trueport=23000connect_timeout=30network_timeout=60heart_beat_interval=30stat_report_interval=60base_path=/opt/fastdfs_storagemax_connections=256buff_size = 256KBaccept_threads=1work_threads=4disk_rw_separated = truedisk_reader_threads = 1disk_writer_threads = 1sync_wait_msec=50sync_interval=0sync_start_time=00:00sync_end_time=23:59write_mark_file_freq=500store_path_count=1store_path0=/opt/fastdfs_storage_datasubdir_count_per_path=256tracker_server=192.168.43.60:22122log_level=inforun_by_group=run_by_user=allow_hosts=*file_distribute_path_mode=0file_distribute_rotate_count=100fsync_after_written_bytes=0sync_log_buff_interval=10sync_binlog_buff_interval=10sync_stat_file_interval=300thread_stack_size=512KBupload_priority=10if_alias_prefix=check_file_duplicate=0file_signature_method=hashkey_namespace=FastDFSkeep_alive=0use_access_log = falserotate_access_log = falseaccess_log_rotate_time=00:00rotate_error_log = falseerror_log_rotate_time=00:00rotate_access_log_size = 0rotate_error_log_size = 0log_file_keep_days = 0file_sync_skip_invalid_record=falseuse_connection_pool = falseconnection_pool_max_idle_time = 3600http.domain_name=http.server_port=8888 修改保存后创建软引用1[root@localhost fdfs]# ln -s /usr/bin/fdfs_storaged /usr/local/bin 3.启动Storage1[root@localhost fdfs]# service fdfs_storaged start 设置开机启动： 1[root@localhost fdfs]# echo &quot;service fdfs_storaged start&quot; |tee -a /etc/rc.d/rc.local 查看一下服务是否启动 123[root@localhost fdfs]# netstat -unltp | grep fdfs tcp 0 0 0.0.0.0:22122 0.0.0.0:* LISTEN 3088/fdfs_trackerd tcp 0 0 0.0.0.0:23000 0.0.0.0:* LISTEN 3139/fdfs_storaged 4.校验整合到这里，fastdfs的东西都已安装完成，最后我们还要确定一下，storage是否注册到了tracker中去。查看命令： 1[root@localhost fdfs]# /usr/bin/fdfs_monitor /etc/fdfs/storage.conf 成功后可以看到：ip_addr = 192.168.43.60 (localhost.localdomain) ACTIVE 四、测试1.配置客户端修改 /etc/fdfs/目录下的client.conf 文件修改要点为： 123base_path=/opt/fastdfs_tracker #tracker服务器文件路径tracker_server=192.168.43.60:22122 #tracker服务器IP地址和端口号http.tracker_server_port=8080 # tracker 服务器的 http端口号，必须和tracker的设置对应起来 完整client.conf 文件信息如下： 1234567891011connect_timeout=30network_timeout=60base_path=/opt/fastdfs_trackertracker_server=192.168.43.60:22122log_level=infouse_connection_pool = falseconnection_pool_max_idle_time = 3600load_fdfs_parameters_from_tracker=falseuse_storage_id = falsestorage_ids_filename = storage_ids.confhttp.tracker_server_port=8080 模拟上传从个人用户目录上传一个图片，进行测试 1[root@localhost fdfs]# fdfs_upload_file /etc/fdfs/client.conf /home/zhangyongliang/9408.jpg #这后面放的是图片的位置 成功后会返回图片存储路径 12[root@localhost fdfs]# fdfs_upload_file /etc/fdfs/client.conf /home/zhangyongliang/9408.jpg group1/M00/00/00/wKgrPFpe9OqAWsHxAAH5yvc2jn8251.jpg 组名：group1磁盘：M00目录：00/00文件名称：wKgrPFpe9OqAWsHxAAH5yvc2jn8251.jpg定位上传的文件位置如下： 1234[root@localhost fdfs]# ll /opt/fastdfs_storage_data/data/00/00/总用量 256-rw-r--r--. 1 root root 129482 1月 17 15:02 wKgrPFpe9OqAWsHxAAH5yvc2jn8251.jpg-rw-r--r--. 1 root root 129482 1月 17 11:53 wKgrPFpeyM2ATkGUAAH5yvc2jn8013.jpg 实际文件存储路径下有创建好的多级目录。data下有256个1级目录，每级目录下又有256个2级子目录，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录中。如果要访问刚上传的图片，我们得需要结合nginx来实现 五、安装Nginx并实现配置1.安装Nginx依赖环境123[root@localhost fdfs]# yum -y install pcre pcre-devel [root@localhost fdfs]# yum -y install zlib zlib-devel [root@localhost fdfs]# yum -y install openssl openssl-devel 2.安装nginx并添加fastdfs-nginx-module解压nginx和fastdfs-nginx-module 12[root@localhost fdfs]# tar -zxvf nginx-1.10.3.tar.gz[root@localhost fdfs]# unzip fastdfs-nginx-module-master.zip 解压后进入nginx目录编译安装nginx,并添加fastdfs-nginx-module 1[root@localhost nginx-1.10.3]# ./configure --prefix=/usr/local/nginx --add-module=/home/zhangyongliang/apps/fastdfs-nginx-module-master/src #解压后fastdfs-nginx-module所在的位置 之后进行编译和安装 1[root@localhost nginx-1.10.3]# make &amp;&amp; make isntall 安装成功后，nginx会安装在/usr/local/nginx,安装后查看 123456789101112[root@localhost src]# ll /usr/local/nginx/总用量 8drwx------. 2 nobody root 6 1月 17 13:23 client_body_tempdrwxr-xr-x. 2 root root 4096 1月 17 13:17 confdrwx------. 2 nobody root 6 1月 17 13:23 fastcgi_tempdrwxr-xr-x. 2 root root 40 1月 17 13:17 htmldrwxr-xr-x. 2 root root 58 1月 17 13:49 logs-rw-r--r--. 1 root root 1156 1月 17 13:29 nginx.confdrwx------. 2 nobody root 6 1月 17 13:23 proxy_tempdrwxr-xr-x. 2 root root 19 1月 17 13:17 sbindrwx------. 2 nobody root 6 1月 17 13:23 scgi_tempdrwx------. 2 nobody root 6 1月 17 13:23 uwsgi_temp 安装成功后，nginx尚未运行时，nginx文件夹没有临时文件夹，例如fastcgi_temp这些文件。 3.配置Storage Nginx修改Nginx 目录下 conf 的配置文件nginx.conf,新增location信息，具体如下： 123456789101112131415161718server &#123; listen 9991; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; location ~/group1/M00 &#123; root /opt/fastdfs_storage/data; ngx_fastdfs_module; &#125; location = /50x.html &#123; root html; &#125;&#125; 然后进入FastDFS安装时的解压过的目录，将http.conf和mime.types拷贝到/etc/fdfs目录下：12[root@localhost src]# cp http.conf mime.types /etc/fdfs/[root@localhost src]# cp mime.types /etc/fdfs/ 另外还需要把fastdfs-nginx-module安装目录中src目录下的mod_fastdfs.conf也拷贝到/etc/fdfs目录下： 1[root@localhost src]# cp mod_fastdfs.conf /etc/fdfs/ 对刚刚拷贝的mod_fastdfs.conf文件进行修改： 123456base_path=/opt/fastdfs_storage #保存日志目录tracker_server=192.168.43.60:22122 #tracker服务器的IP地址以及端口号storage_server_port=23000 #storage服务器的端口号url_have_group_name = true #文件 url 中是否有 group 名store_path0=/opt/fastdfs_storage_data #存储路径group_count = 1 #设置组的个数 在文件的最后，设置group12345[group1]group_name=group1storage_server_port=23000store_path_count=1store_path0=/opt/fastdfs_storage_data 创建M00至storage存储目录的符号连接： 1ln -s /opt/fastdfs_storage_data/data/ /opt/fastdfs_storage_data/data/M00 启动Nginx： 1[root@localhost src]# /usr/local/nginx/sbin/nginx 访问Nginx是否启动 1234567891011121314151617181920212223242526[root@localhost src]# curl localhost:9991&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 4.配置Tracker Nginx在nginx.conf 文件添加一个虚拟机 1234567891011121314upstream fdfs_group1 &#123; server 127.0.0.1:9991; &#125; server &#123; listen 80; server_name localhost; location /group1/M00 &#123; proxy_pass http://fdfs_group1; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 完整nginx.conf配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 9991; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; location ~/group1/M00 &#123; root /opt/fastdfs_storage/data; ngx_fastdfs_module; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; upstream fdfs_group1 &#123; server 127.0.0.1:9991; &#125; server &#123; listen 80; server_name localhost; location /group1/M00 &#123; proxy_pass http://fdfs_group1; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; 将Nginx重新启动 1[root@localhost src]# /usr/local/nginx/sbin/nginx -s reload 访问Nginx是否已经启动 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@localhost src]# curl localhost:9991&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;[root@localhost src]# curl localhost&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 5.HTTP访问图片资源 访问图片资源路径http://192.168.43.60/group1/M00/00/00/wKgrPFpeyM2ATkGUAAH5yvc2jn8013.jpg FastDFS资源访问截图 可以看到已上传的图片，说明你已成功实现了FastDFS单机的文件系统搭建。 补充说明：如果Tracker 服务、Storage服务、Nginx服务开机后没有自启动，请执行一下操作并进行重启 12[root@localhost ~]# chkconfig --add fdfs_trackerd[root@localhost ~]# chkconfig --add fdfs_storaged 编辑目录下的/etc/rc.d/rc.local,内容如下：1234567891011121314#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure# that this script will be executed during boot.touch /var/lock/subsys/local/usr/local/nginx/sbin/nginx 主要增加了Nginx的启动，之后进行文件生效，重新启动系统 123[root@localhost ~]# chmod +x /etc/rc.d/rc.local[root@localhost ~]# source /etc/rc.d/rc.local [root@localhost ~]# reboot]]></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7.2搭建FastDFS 分布式文件系统，实现高可用集群]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FCentOS_7.2%E6%90%AD%E5%BB%BAFastDFS_%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%EF%BC%8C%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[分布式集群搭建结构 双Tracker 2组Group 轮询存储策略 Keepalived+Nginx高可用 Nginx缓存 4个存储节点 一、 集群规划清单1.安装清单 软件名称 版本 百度云盘存放名称 FastDFS 5.11 fastdfs-5.11.zip FastDFS-Nginx-module 无 fastdfs-nginx-module-master.zip LibFastCommon 1.0.36 libfastcommon-1.0.36.zip nginx 1.10.3 nginx-1.10.3.tar.gz nginx-pure-cache 2.3 ngx_cache_purge-2.3.tar.gz 安装所需文件均上传到百度云盘，位置：FastDFS百度云盘 2.集群规划 虚拟机 IP 说明 Keepalived+Nginx1[Master] 192.168.43.101 Nginx Server 01 Keeepalived+Nginx[Backup] 192.168.43.102 Nginx Server 02 VIP 192.168.43.150 虚拟漂移IP Tracker01 192.168.43.70 Tracker01服务器 Tracker02 192.168.43.71 Tracker02服务器 Storage01 192.168.43.72 Storage01服务器【group1】 Storage02 192.168.43.73 Storage02服务器【group1】 Storage03 192.168.43.74 Storage03服务器【group2】 Storage04 192.168.43.75 Storage04服务器【group2】 整体架构图如下图所示： 图片来源：CSDN作者 liuyazhuang 二、集群安装以下操作均在关闭所有节点防火墙进行的，请根据个人情况开启相关端口或关闭防火墙 1.安装LibFastCommon/FastDFS模块执行节点 Tracker01、Tracker02、Storage01、Storage03、Storage041[root@localhost fastDFS]# unzip libfastcommon-1.0.36.zip 解压后目录如下：1234567891011[root@localhost fastdfs-5.11]# ll[root@localhost libfastcommon-1.0.36]# ll总用量 32drwxr-xr-x. 2 root root 117 4月 5 2017 doc-rw-r--r--. 1 root root 8005 4月 5 2017 HISTORY-rw-r--r--. 1 root root 566 4月 5 2017 INSTALL-rw-r--r--. 1 root root 1606 4月 5 2017 libfastcommon.spec-rwxr-xr-x. 1 root root 3099 4月 5 2017 make.shdrwxr-xr-x. 2 root root 191 4月 5 2017 php-fastcommon-rw-r--r--. 1 root root 2763 4月 5 2017 READMEdrwxr-xr-x. 3 root root 4096 1月 17 11:21 src 安装C编译工具 gcc 1[root@localhost fastdfs-5.11]# yum -y install gcc-c++ 安装装LibFastCommon 编译libfastcommon软件并安装 1[root@localhost fastdfs-5.11]# ./make.sh &amp;&amp; ./make.sh install 为libcommon 创建软链接到/usr/local/lib目录下 1234[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so[root@localhost fastdfs-5.11]# ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so 安装FastDFS解压安装包1[root@localhost fastDFS]# unzip fastdfs-5.11.zip 进入解压目录并进行编译和安装 12[root@localhost fastDFS]# cd fastdfs-5.11/[root@localhost fastdfs-5.11]# ./make.sh &amp;&amp; ./make.sh install 安装成功后，FastDFS会安装在/etc/fdfs目录下： 123456789101112[root@localhost fastdfs-5.11]# ll /etc/fdfs/总用量 76-rw-r--r--. 1 root root 316 1月 17 11:47 client.conf-rw-r--r--. 1 root root 1461 1月 17 11:25 client.conf.sample-rw-r--r--. 1 root root 955 1月 17 13:20 http.conf-rw-r--r--. 1 root root 31172 1月 17 13:21 mime.types-rw-r--r--. 1 root root 3716 1月 17 12:57 mod_fastdfs.conf-rw-r--r--. 1 root root 1278 1月 17 11:40 storage.conf-rw-r--r--. 1 root root 7927 1月 17 11:25 storage.conf.sample-rw-r--r--. 1 root root 105 1月 17 11:25 storage_ids.conf.sample-rw-r--r--. 1 root root 1356 1月 17 11:34 tracker.conf-rw-r--r--. 1 root root 7389 1月 17 11:25 tracker.conf.sample 我们需要把这三个示例文件复制一份，去掉.sample 123[root@localhost fdfs]# cp client.conf.sample client.conf[root@localhost fdfs]# cp storage.conf.sample storage.conf[root@localhost fdfs]# cp tracker.conf.sample tracker.conf FastDFS安装结束 2.安装Tracker并实现节点信息配置执行节点 Tracker01、Tracker02 创建tracker工作目录 此目录用于保存tracker 的data和log 1[root@localhost fdfs]# mkdir /opt/fastdfs_tracker 配置tracker 配置 /etc/fdfs目录下tracker.conf主要实现以下5个配置内容： 123451.disabled=false 2.port=22122 #默认端口号 3.base_path=/opt/fastdfs_tracker #我刚刚创建的目录 4.http.server_port=8080 #默认端口是80805.store_lookup=0 #采用轮询策略进行存储，0 轮询 1：始终定向到某个group 2：负载进行存储文件 完整tracker.conf 文件信息如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103disabled=falsebind_addr= 0.0.0.0port=22122connect_timeout=30network_timeout=60base_path=/opt/fastdfs_trackermax_connections=512accept_threads=1work_threads=4min_buff_size = 8KBmax_buff_size = 128KBstore_lookup=0store_group=group2store_server=0store_path=0download_server=0reserved_storage_space = 10%log_level=inforun_by_group=run_by_user=allow_hosts=*sync_log_buff_interval = 10check_active_interval = 120thread_stack_size = 64KBstorage_ip_changed_auto_adjust = truestorage_sync_file_max_delay = 86400storage_sync_file_max_time = 300use_trunk_file = false slot_min_size = 256slot_max_size = 16MBtrunk_file_size = 64MBtrunk_create_file_advance = falsetrunk_create_file_time_base = 02:00trunk_create_file_interval = 86400trunk_create_file_space_threshold = 20Gtrunk_init_check_occupying = falsetrunk_init_reload_from_binlog = falsetrunk_compress_binlog_min_interval = 0use_storage_id = falsestorage_ids_filename = storage_ids.confid_type_in_filename = ipstore_slave_file_use_link = falserotate_error_log = falseerror_log_rotate_time=00:00rotate_error_log_size = 0log_file_keep_days = 0use_connection_pool = falseconnection_pool_max_idle_time = 3600http.server_port=8080http.check_alive_interval=30http.check_alive_type=tcphttp.check_alive_uri=/status.html 修改保存后创建软引用 1[root@localhost fdfs]# ln -s /usr/bin/fdfs_storaged /usr/local/bin 启动tracker，并加入开机启动项 1[root@localhost fdfs]# service fdfs_trackerd start 将tracker加入开机启动项 1[root@localhost fdfs]# echo &quot;service fdfs_trackerd start&quot; |tee -a /etc/rc.d/rc.local 3.安装Storage模块并实现配置执行节点 Storage01、Storage02、Storage03、Storage04 建立存储目录 在存储各节点建了两个目录fastdfs_storage_data,fastdfs_storage 1234567[root@localhost opt]# mkdir fastdfs_storage[root@localhost opt]# mkdir fastdfs_storage_data[root@localhost opt]# ll总用量 0drwxr-xr-x. 4 root root 30 1月 17 11:45 fastdfs_storagedrwxr-xr-x. 3 root root 18 1月 17 11:45 fastdfs_storage_datadrwxr-xr-x. 4 root root 30 1月 17 11:35 fastdfs_tracker 修改存储节点目录下/etc/fdfs/storage.conf配置信息，具体如下： 123456789disabled=false #启用配置文件 group_name=group1 #组名（第一组为 group1， 第二组为 group2） port=23000 #storage 的端口号,同一个组的 storage 端口号必须相同 base_path=/opt/fastdfs_storage #设置storage数据文件和日志目录 store_path0=/opt/fastdfs_storage_data #实际文件存储路径 store_path_count=1 #存储路径个数，需要和 store_path 个数匹配 tracker_server=192.168.43.70:22122 #tracker 服务器的 IP 地址和端口 tracker_server=192.168.43.70:22122 #多个 tracker 直接添加多条配置 http.server_port=8888 #设置 http 端口号 完整配置信息如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657disabled=falsegroup_name=group1bind_addr=client_bind=trueport=23000connect_timeout=30network_timeout=60heart_beat_interval=30stat_report_interval=60base_path=/opt/fastdfs_storagemax_connections=256buff_size = 256KBaccept_threads=1work_threads=4disk_rw_separated = truedisk_reader_threads = 1disk_writer_threads = 1sync_wait_msec=50sync_interval=0sync_start_time=00:00sync_end_time=23:59write_mark_file_freq=500store_path_count=1store_path0=/opt/fastdfs_storage_datasubdir_count_per_path=256tracker_server=192.168.43.70:22122tracker_server=192.168.43.71:22122log_level=inforun_by_group=run_by_user=allow_hosts=*file_distribute_path_mode=0file_distribute_rotate_count=100fsync_after_written_bytes=0sync_log_buff_interval=10sync_binlog_buff_interval=10sync_stat_file_interval=300thread_stack_size=512KBupload_priority=10if_alias_prefix=check_file_duplicate=0file_signature_method=hashkey_namespace=FastDFSkeep_alive=0use_access_log = falserotate_access_log = falseaccess_log_rotate_time=00:00rotate_error_log = falseerror_log_rotate_time=00:00rotate_access_log_size = 0rotate_error_log_size = 0log_file_keep_days = 0file_sync_skip_invalid_record=falseuse_connection_pool = falseconnection_pool_max_idle_time = 3600http.domain_name=http.server_port=8888 启动Storage各节点配置好信息好之后，启动Storage 1[root@localhost fdfs]# service fdfs_storaged start 启动后查看日志情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286[root@localhost fdfs]# fdfs_monitor /etc/fdfs/storage.conf[2018-01-20 16:56:48] DEBUG - base_path=/opt/fastdfs_storage, connect_timeout=30, network_timeout=60, tracker_server_count=2, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0server_count=2, server_index=0tracker server is 192.168.43.70:22122group count: 2Group 1:group name = group1disk total space = 47073 MBdisk free space = 35162 MBtrunk free space = 0 MBstorage server count = 2active server count = 2storage server port = 23000storage HTTP port = 8888store path count = 1subdir count per path = 256current write server index = 0current trunk file id = 0 Storage 1: id = 192.168.43.72 ip_addr = 192.168.43.72 (localhost.localdomain) ACTIVE http domain = version = 5.11 join time = 2018-01-19 13:59:30 up time = 2018-01-20 12:37:18 total storage = 47073 MB free storage = 35162 MB upload priority = 10 store_path_count = 1 subdir_count_per_path = 256 storage_port = 23000 storage_http_port = 8888 current_write_path = 0 source storage id = if_trunk_server = 0 connection.alloc_count = 256 connection.current_count = 1 connection.max_count = 1 total_upload_count = 3 success_upload_count = 3 total_append_count = 0 success_append_count = 0 total_modify_count = 0 success_modify_count = 0 total_truncate_count = 0 success_truncate_count = 0 total_set_meta_count = 0 success_set_meta_count = 0 total_delete_count = 0 success_delete_count = 0 total_download_count = 0 success_download_count = 0 total_get_meta_count = 0 success_get_meta_count = 0 total_create_link_count = 0 success_create_link_count = 0 total_delete_link_count = 0 success_delete_link_count = 0 total_upload_bytes = 791904 success_upload_bytes = 791904 total_append_bytes = 0 success_append_bytes = 0 total_modify_bytes = 0 success_modify_bytes = 0 stotal_download_bytes = 0 success_download_bytes = 0 total_sync_in_bytes = 775234 success_sync_in_bytes = 775234 total_sync_out_bytes = 0 success_sync_out_bytes = 0 total_file_open_count = 4 success_file_open_count = 4 total_file_read_count = 0 success_file_read_count = 0 total_file_write_count = 8 success_file_write_count = 8 last_heart_beat_time = 2018-01-20 16:56:18 last_source_update = 2018-01-19 19:34:55 last_sync_update = 2018-01-19 15:28:56 last_synced_timestamp = 2018-01-19 15:28:48 (0s delay) Storage 2: id = 192.168.43.73 ip_addr = 192.168.43.73 ACTIVE http domain = version = 5.11 join time = 2018-01-19 14:00:21 up time = 2018-01-20 12:37:42 total storage = 47073 MB free storage = 35166 MB upload priority = 10 store_path_count = 1 subdir_count_per_path = 256 storage_port = 23000 storage_http_port = 8888 current_write_path = 0 source storage id = 192.168.43.72 if_trunk_server = 0 connection.alloc_count = 256 connection.current_count = 1 connection.max_count = 1 total_upload_count = 1 success_upload_count = 1 total_append_count = 0 success_append_count = 0 total_modify_count = 0 success_modify_count = 0 total_truncate_count = 0 success_truncate_count = 0 total_set_meta_count = 0 success_set_meta_count = 0 total_delete_count = 0 success_delete_count = 0 total_download_count = 0 success_download_count = 0 total_get_meta_count = 0 success_get_meta_count = 0 total_create_link_count = 0 success_create_link_count = 0 total_delete_link_count = 0 success_delete_link_count = 0 total_upload_bytes = 775234 success_upload_bytes = 775234 total_append_bytes = 0 success_append_bytes = 0 total_modify_bytes = 0 success_modify_bytes = 0 stotal_download_bytes = 0 success_download_bytes = 0 total_sync_in_bytes = 791904 success_sync_in_bytes = 791904 total_sync_out_bytes = 0 success_sync_out_bytes = 0 total_file_open_count = 4 success_file_open_count = 4 total_file_read_count = 0 success_file_read_count = 0 total_file_write_count = 8 success_file_write_count = 8 last_heart_beat_time = 2018-01-20 16:56:42 last_source_update = 2018-01-19 15:28:48 last_sync_update = 2018-01-19 19:34:59 last_synced_timestamp = 2018-01-19 19:34:55 (0s delay)Group 2:group name = group2disk total space = 47073 MBdisk free space = 35165 MBtrunk free space = 0 MBstorage server count = 2active server count = 2storage server port = 23000storage HTTP port = 8888store path count = 1subdir count per path = 256current write server index = 0current trunk file id = 0 Storage 1: id = 192.168.43.74 ip_addr = 192.168.43.74 ACTIVE http domain = version = 5.11 join time = 2018-01-19 14:01:05 up time = 2018-01-20 12:38:00 total storage = 47073 MB free storage = 35165 MB upload priority = 10 store_path_count = 1 subdir_count_per_path = 256 storage_port = 23000 storage_http_port = 8888 current_write_path = 0 source storage id = if_trunk_server = 0 connection.alloc_count = 256 connection.current_count = 1 connection.max_count = 1 total_upload_count = 4 success_upload_count = 4 total_append_count = 0 success_append_count = 0 total_modify_count = 0 success_modify_count = 0 total_truncate_count = 0 success_truncate_count = 0 total_set_meta_count = 0 success_set_meta_count = 0 total_delete_count = 0 success_delete_count = 0 total_download_count = 0 success_download_count = 0 total_get_meta_count = 0 success_get_meta_count = 0 total_create_link_count = 0 success_create_link_count = 0 total_delete_link_count = 0 success_delete_link_count = 0 total_upload_bytes = 2107770 success_upload_bytes = 2107770 total_append_bytes = 0 success_append_bytes = 0 total_modify_bytes = 0 success_modify_bytes = 0 stotal_download_bytes = 0 success_download_bytes = 0 total_sync_in_bytes = 1550468 success_sync_in_bytes = 1550468 total_sync_out_bytes = 0 success_sync_out_bytes = 0 total_file_open_count = 6 success_file_open_count = 6 total_file_read_count = 0 success_file_read_count = 0 total_file_write_count = 15 success_file_write_count = 15 last_heart_beat_time = 2018-01-20 16:56:38 last_source_update = 2018-01-19 19:35:40 last_sync_update = 2018-01-19 15:28:53 last_synced_timestamp = 2018-01-19 15:28:50 (-1s delay) Storage 2: id = 192.168.43.75 ip_addr = 192.168.43.75 ACTIVE http domain = version = 5.11 join time = 2018-01-19 14:01:27 up time = 2018-01-20 12:38:20 total storage = 47073 MB free storage = 35165 MB upload priority = 10 store_path_count = 1 subdir_count_per_path = 256 storage_port = 23000 storage_http_port = 8888 current_write_path = 0 source storage id = 192.168.43.74 if_trunk_server = 0 connection.alloc_count = 256 connection.current_count = 1 connection.max_count = 1 total_upload_count = 2 success_upload_count = 2 total_append_count = 0 success_append_count = 0 total_modify_count = 0 success_modify_count = 0 total_truncate_count = 0 success_truncate_count = 0 total_set_meta_count = 0 success_set_meta_count = 0 total_delete_count = 0 success_delete_count = 0 total_download_count = 0 success_download_count = 0 total_get_meta_count = 0 success_get_meta_count = 0 total_create_link_count = 0 success_create_link_count = 0 total_delete_link_count = 0 success_delete_link_count = 0 total_upload_bytes = 1550468 success_upload_bytes = 1550468 total_append_bytes = 0 success_append_bytes = 0 total_modify_bytes = 0 success_modify_bytes = 0 stotal_download_bytes = 0 success_download_bytes = 0 total_sync_in_bytes = 2107770 success_sync_in_bytes = 2107770 total_sync_out_bytes = 0 success_sync_out_bytes = 0 total_file_open_count = 6 success_file_open_count = 6 total_file_read_count = 0 success_file_read_count = 0 total_file_write_count = 15 success_file_write_count = 15 last_heart_beat_time = 2018-01-20 16:56:23 last_source_update = 2018-01-19 15:28:49 last_sync_update = 2018-01-19 19:35:46 last_synced_timestamp = 2018-01-19 19:35:40 (0s delay) 如果看到有2组Storage信息，则表示配置信息配置成功，并注册到Tracker中，查看日志启动情况 1234567891011[root@localhost fdfs]# tail -f /opt/fastdfs_storage/logs/storaged.log [2018-01-20 12:37:18] INFO - FastDFS v5.11, base_path=/opt/fastdfs_storage, store_path_count=1, subdir_count_per_path=256, group_name=group1, run_by_group=, run_by_user=, connect_timeout=30s, network_timeout=60s, port=23000, bind_addr=, client_bind=1, max_connections=256, accept_threads=1, work_threads=4, disk_rw_separated=1, disk_reader_threads=1, disk_writer_threads=1, buff_size=256KB, heart_beat_interval=30s, stat_report_interval=60s, tracker_server_count=2, sync_wait_msec=50ms, sync_interval=0ms, sync_start_time=00:00, sync_end_time=23:59, write_mark_file_freq=500, allow_ip_count=-1, file_distribute_path_mode=0, file_distribute_rotate_count=100, fsync_after_written_bytes=0, sync_log_buff_interval=10s, sync_binlog_buff_interval=10s, sync_stat_file_interval=300s, thread_stack_size=512 KB, upload_priority=10, if_alias_prefix=, check_file_duplicate=0, file_signature_method=hash, FDHT group count=0, FDHT server count=0, FDHT key_namespace=, FDHT keep_alive=0, HTTP server port=8888, domain name=, use_access_log=0, rotate_access_log=0, access_log_rotate_time=00:00, rotate_error_log=0, error_log_rotate_time=00:00, rotate_access_log_size=0, rotate_error_log_size=0, log_file_keep_days=0, file_sync_skip_invalid_record=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s[2018-01-20 12:37:18] INFO - file: storage_param_getter.c, line: 191, use_storage_id=0, id_type_in_filename=ip, storage_ip_changed_auto_adjust=1, store_path=0, reserved_storage_space=10.00%, use_trunk_file=0, slot_min_size=256, slot_max_size=16 MB, trunk_file_size=64 MB, trunk_create_file_advance=0, trunk_create_file_time_base=02:00, trunk_create_file_interval=86400, trunk_create_file_space_threshold=20 GB, trunk_init_check_occupying=0, trunk_init_reload_from_binlog=0, trunk_compress_binlog_min_interval=0, store_slave_file_use_link=0[2018-01-20 12:37:18] INFO - file: storage_func.c, line: 257, tracker_client_ip: 192.168.43.72, my_server_id_str: 192.168.43.72, g_server_id_in_filename: 1210820800[2018-01-20 12:37:18] INFO - file: tracker_client_thread.c, line: 310, successfully connect to tracker server 192.168.43.71:22122, as a tracker client, my ip is 192.168.43.72[2018-01-20 12:37:18] INFO - file: tracker_client_thread.c, line: 1947, tracker server: #0. 192.168.43.70:22122, my_report_status: -1[2018-01-20 12:37:18] INFO - file: tracker_client_thread.c, line: 310, successfully connect to tracker server 192.168.43.70:22122, as a tracker client, my ip is 192.168.43.72[2018-01-20 12:37:18] INFO - file: tracker_client_thread.c, line: 1947, tracker server: #0. 192.168.43.70:22122, my_report_status: -1[2018-01-20 12:37:48] INFO - file: tracker_client_thread.c, line: 1263, tracker server 192.168.43.71:22122, set tracker leader: 192.168.43.71:22122[2018-01-20 12:37:48] INFO - file: storage_sync.c, line: 2732, successfully connect to storage server 192.168.43.73:23000 发现此时192.168.43.71作为Tracker的Leader。 设置Storage开机自启动 1[root@localhost fdfs]# echo &quot;service fdfs_storaged start&quot; |tee -a /etc/rc.d/rc.local 安装fastdfs-nginx-module、Nginx模块 安装Nginx模块所需的依赖环境 123[root@localhost fdfs]# yum -y install pcre pcre-devel [root@localhost fdfs]# yum -y install zlib zlib-devel [root@localhost fdfs]# yum -y install openssl openssl-devel 解压nginx和fastdfs-nginx-module12[root@localhost fdfs]# tar -zxvf nginx-1.10.3.tar.gz[root@localhost fdfs]# unzip fastdfs-nginx-module-master.zip 进入Nginx解压目录进行编译安装1[root@localhost nginx-1.10.3]# ./configure --prefix=/usr/local/nginx --add-module=/home/zhangyongliang/apps/fastdfs-nginx-module-master/src #解压后fastdfs-nginx-module所在的位置 安装成功后，nginx会安装在/usr/local/nginx,安装后查看 123456789101112[root@localhost src]# ll /usr/local/nginx/总用量 8drwx------. 2 nobody root 6 1月 17 13:23 client_body_tempdrwxr-xr-x. 2 root root 4096 1月 17 13:17 confdrwx------. 2 nobody root 6 1月 17 13:23 fastcgi_tempdrwxr-xr-x. 2 root root 40 1月 17 13:17 htmldrwxr-xr-x. 2 root root 58 1月 17 13:49 logs-rw-r--r--. 1 root root 1156 1月 17 13:29 nginx.confdrwx------. 2 nobody root 6 1月 17 13:23 proxy_tempdrwxr-xr-x. 2 root root 19 1月 17 13:17 sbindrwx------. 2 nobody root 6 1月 17 13:23 scgi_tempdrwx------. 2 nobody root 6 1月 17 13:23 uwsgi_temp 安装成功后，nginx尚未运行时，nginx文件夹没有临时文件夹，例如fastcgi_temp这些文件。复制 fastdfs-nginx-module 源码中的配置文件到/etc/fdfs 目录， 并修改 12[root@localhost src]# cp /usr/local/src/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/ [root@localhost src]# vi /etc/fdfs/mod_fastdfs.conf 12345678910111213141516171819202122(1)第1组 Storage 的 mod_fastdfs.conf 配置如下： connect_timeout=10 base_path=/opt/fastdfs_storagetracker_server=192.168.1.131:22122 tracker_server=192.168.1.132:22122 storage_server_port=23000 group_name=group1 url_have_group_name = true store_path0=/opt/fastdfs_storage_datagroup_count = 2 [group1] group_name=group1 storage_server_port=23000 store_path_count=1 store_path0=/opt/fastdfs_storage_data[group2] group_name=group2 storage_server_port=23000 store_path_count=1 store_path0=/opt/fastdfs_storage_data (2)第2组 Storage 的 mod_fastdfs.conf 配置与第一组配置只有 group_name 不同： group_name=group2 完整信息如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# connect timeout in seconds# default value is 30sconnect_timeout=2# network recv and send timeout in seconds# default value is 30snetwork_timeout=30# the base path to store log filesbase_path=/opt/fastdfs_storage# if load FastDFS parameters from tracker server# since V1.12# default value is falseload_fdfs_parameters_from_tracker=true# storage sync file max delay seconds# same as tracker.conf# valid only when load_fdfs_parameters_from_tracker is false# since V1.12# default value is 86400 seconds (one day)storage_sync_file_max_delay = 86400# if use storage ID instead of IP address# same as tracker.conf# valid only when load_fdfs_parameters_from_tracker is false# default value is false# since V1.13use_storage_id = false# specify storage ids filename, can use relative or absolute path# same as tracker.conf# valid only when load_fdfs_parameters_from_tracker is false# since V1.13storage_ids_filename = storage_ids.conf# FastDFS tracker_server can ocur more than once, and tracker_server format is# &quot;host:port&quot;, host can be hostname or ip address# valid only when load_fdfs_parameters_from_tracker is truetracker_server=192.168.43.70:22122tracker_server=192.168.43.71:22122# the port of the local storage server# the default value is 23000storage_server_port=23000# the group name of the local storage servergroup_name=group1# if the url / uri including the group name# set to false when uri like /M00/00/00/xxx# set to true when uri like $&#123;group_name&#125;/M00/00/00/xxx, such as group1/M00/xxx# default value is falseurl_have_group_name = true# path(disk or mount point) count, default value is 1# must same as storage.confstore_path_count=1# store_path#, based 0, if store_path0 not exists, it&apos;s value is base_path# the paths must be exist# must same as storage.confstore_path0=/opt/fastdfs_storage_data#store_path1=/home/yuqing/fastdfs1# standard log level as syslog, case insensitive, value list:### emerg for emergency### alert### crit for critical### error### warn for warning### notice### info### debuglog_level=info# set the log filename, such as /usr/local/apache2/logs/mod_fastdfs.log# empty for output to stderr (apache and nginx error_log file)log_filename=# response mode when the file not exist in the local file system## proxy: get the content from other storage server, then send to client## redirect: redirect to the original storage server (HTTP Header is Location)response_mode=proxy# the NIC alias prefix, such as eth in Linux, you can see it by ifconfig -a# multi aliases split by comma. empty value means auto set by OS type# this paramter used to get all ip address of the local host# default values is emptyif_alias_prefix=# use &quot;#include&quot; directive to include HTTP config file# NOTE: #include is an include directive, do NOT remove the # before include#include http.conf# if support flv# default value is false# since v1.15flv_support = true# flv file extension name# default value is flv# since v1.15flv_extension = flv# set the group count# set to none zero to support multi-group on this storage server# set to 0 for single group only# groups settings section as [group1], [group2], ..., [groupN]# default value is 0# since v1.14group_count = 2# group settings for group #1# since v1.14# when support multi-group on this storage server, uncomment following section[group1]group_name=group1storage_server_port=23000store_path_count=1store_path0=/opt/fastdfs_storage_data[group2]group_name=group2storage_server_port=23000store_path_count=1store_path0=/opt/fastdfs_storage_data# group settings for group #2# since v1.14# when support multi-group, uncomment following section as neccessary#[group2]#group_name=group2#storage_server_port=23000#store_path_count=1#store_path0=/home/yuqing/fastdfs 复制 FastDFS 安装目录的部分配置文件到/etc/fdfs 目录 123[root@localhost conf]# pwd/home/zhangyongliang/apps/fastdfs-5.11/conf[root@localhost conf]# cp http.conf mime.types /etc/fdfs/ 创建M00至storage存储目录的符号连接： 1ln -s /opt/fastdfs_storage_data/data/ /opt/fastdfs_storage_data/data/M00 配置 Nginx， 简洁版 nginx 配置样例 123456789101112131415161718192021222324# vi /usr/local/nginx/conf/nginx.conf user root; worker_processes 1; events &#123; worker_connections 1024; &#125; http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 8888; server_name localhost; location ~/group([0-9])/M00 &#123; #alias /fastdfs/storage/data; ngx_fastdfs_module; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; &#125; 注意、 说明： 8888 端口值是要与/etc/fdfs/storage.conf 中的 http.server_port=8888 相对应，因为 http.server_port 默认为 8888,如果想改成 80，则要对应修改过来。重新启动各节点的Nginx服务1[root@localhost conf]#/usr/local/nginx/sbin/nginx -s reload 4.文件上传测试执行节点Tracker01、Tracker02修改 Tracker 服务器中的客户端配置文件1234# vi /etc/fdfs/client.conf base_path=/fastdfs/tracker tracker_server=192.168.43.70:22122 tracker_server=192.168.43.71:22122 执行如下文件上传命令 1234[root@localhost zhangyongliang]# fdfs_upload_file /etc/fdfs/client.conf P71022-205803.jpg group1/M00/00/00/wKgrSFpjC26AH1g2AAvUQrxXbkA557.jpg[root@localhost zhangyongliang]# fdfs_upload_file /etc/fdfs/client.conf P71022-205803.jpg group2/M00/00/00/wKgrSlpjC3aAARrXAAvUQrxXbkA048.jpg 进行2次上传后，发现文件被均衡分到2个group。 5.Tracker安装Nginx、 ngx_cache_purge 模块 安装编译 Nginx 所需的依赖包1[root@localhost zhangyongliang]# yum install gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl openssl-devel 解压Nginx和ngx_cache_pure模块 12[root@localhost apps]# tar ngx_cache_purge-2.3.tar.gz[root@localhost apps]# tar nginx-1.10.3.tar.gz 编译安装 Nginx（添加 ngx_cache_purge 模块） 123[root@localhost apps]# cd nginx-1.13.0 [root@localhost nginx-1.13.0# ./configure --prefix=/usr/local/nginx --add-module=/usr/local/src/ngx_cache_purge-2.3 [root@localhost nginx-1.13.0]# make &amp;&amp; make install 配置 Nginx， 设置负载均衡以及缓存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172# vi /usr/local/nginx/conf/nginx.conf #user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; tcp_nopush on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; #设置缓存 server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 300m; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; #设置缓存存储路径，存储方式，分别内存大小，磁盘最大空间，缓存期限 proxy_cache_path /opt/fastdfs_tracker/proxy_cache levels=1:2 keys_zone=http-cache:200m max_size=1g inactive=30d; proxy_temp_path /opt/fastdfs_tracker/tmp; #group1的服务设置 upstream fdfs_group1 &#123; server 192.168.43.72:8888 weight=1 max_fails=2 fail_timeout=30s; server 192.168.43.73:8888 weight=1 max_fails=2 fail_timeout=30s; &#125; #group2的服务设置 upstream fdfs_group2 &#123; server 192.168.43.74:8888 weight=1 max_fails=2 fail_timeout=30s; server 192.168.43.75:8888 weight=1 max_fails=2 fail_timeout=30s; &#125; server &#123; listen 8000; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; #group1的负载均衡配置 location /group1/M00 &#123; proxy_next_upstream http_502 http_504 error timeout invalid_header; proxy_cache http-cache; proxy_cache_valid 200 304 12h; proxy_cache_key $uri$is_args$args; #对应group1的服务设置 proxy_pass http://fdfs_group1; expires 30d; &#125; location /group2/M00 &#123; proxy_next_upstream http_502 http_504 error timeout invalid_header; proxy_cache http-cache; proxy_cache_valid 200 304 12h; proxy_cache_key $uri$is_args$args; #对应group2的服务设置 proxy_pass http://fdfs_group2; expires 30d; &#125; location ~/purge(/.*) &#123; allow 127.0.0.1; allow 192.168.43.0/24; deny all; proxy_cache_purge http-cache $1$is_args$args; &#125; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; 根据Nginx配置，创建对应目录下的文件夹 1234567[root@localhost fastdfs_tracker]# mkdir proxy_cache tmp[root@localhost fastdfs_tracker]# ll总用量 0drwxr-xr-x. 2 root root 178 1月 20 12:37 datadrwxr-xr-x. 2 root root 26 1月 19 12:01 logsdrwxr-xr-x. 7 nobody root 51 1月 19 19:35 proxy_cachedrwxr-xr-x. 2 nobody root 6 1月 19 19:35 tmp 重启Nginx进行访问测试 12重启 Nginx [root@localhost fastdfs_tracker]# /usr/local/nginx/sbin/nginx -s reload 前面直接通过访问 Storage 节点中的 Nginx 的文件http://192.168.43.72:8888/group1/M00/00/00/wKgrSFpjC26AH1g2AAvUQrxXbkA557.jpg]http://192.168.43.74:8888/group2/M00/00/00/wKgrSlpjC3aAARrXAAvUQrxXbkA048.jpg现在可以通过 Tracker 中的 Nginx 来进行访问(1)通过 Tracker1 中的 Nginx 来访问http://192.168.43.70:8000/group1/M00/00/00/wKgrSFpjC26AH1g2AAvUQrxXbkA557.jpghttp://192.168.43.70:8000/group2/M00/00/00/wKgrSlpjC3aAARrXAAvUQrxXbkA048.jpg(2)通过 Tracker2 中的 Nginx 来访问http://192.168.43.71:8000/group1/M00/00/00/wKgrSFpjC26AH1g2AAvUQrxXbkA557.jpghttp://192.168.50.71:8000/group2/M00/00/00/wKgrSlpjC3aAARrXAAvUQrxXbkA048.jpg 6.构建Keepalive+Nginx 实现虚拟IP的代理关于使用Keepalive+Nginx进行代理的环境安装，请参考本人简书此文：Keepalived+Nginx+Tomcat 实现高可用Web集群 本文不再做赘述说明 启动Keepalvie+nginx Master主节点【192.168.43.101】 启动Keepalvie+nginx BackUp备节点【192.168.43.102】修改2个节点Nginx下目录的nginx.conf的配置文件信息，添加如下内容主要内容为2个Tracker加入到Keepalive+nginx代理当中1234upstream fastdfs_tracker &#123; server 192.168.43.70:8000 weight=1 max_fails=2 fail_timeout=30s; server 192.168.43.71:8000 weight=1 max_fails=2 fail_timeout=30s; &#125; 第二处修改是添加了一个location并且匹配规则是路径当中有fastdfs 1234567891011location /fastdfs &#123; root html; index index.html index.htm; proxy_pass http://fastdfs_tracker/; proxy_set_header Host $http_host; proxy_set_header Cookie $http_cookie; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 300m; &#125; 完整配置代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; upstream fastdfs_tracker &#123; server 192.168.43.70:8000 weight=1 max_fails=2 fail_timeout=30s; server 192.168.43.71:8000 weight=1 max_fails=2 fail_timeout=30s; &#125; upstream tomcat&#123; server 192.168.43.103:8080 weight=1; server 192.168.43.104:8080 weight=1; &#125; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; proxy_pass http://tomcat; proxy_set_header X-NGINX &quot;NGINX-1&quot;; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location /fastdfs &#123; root html; index index.html index.htm; proxy_pass http://fastdfs_tracker/; proxy_set_header Host $http_host; proxy_set_header Cookie $http_cookie; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 300m; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; 修改之后，重新启动Keepalive+Nginx2台主备节点。 1[root@nginx1 conf]# /usr/local/nginx/sbin/nginx -s reload 我们现在就用虚拟IP192.168.43.150来访问我们刚才上传的图片，只是注意在地址栏中要记得输入fastdfs（这是我们nginx.conf文件中location /fastdfs{}规则规定的）。如下图所示，发现，我们通过虚拟IP便可以访问我们上传的图片了。这样的好处是，对用户来说，只需要访问这个虚拟IP就可以了，不用关心FastDFS集群内部的转发机制。 集群VIP访问截图.png 至此，分布式文件系统就搭建完成了，在通过Java访问时，只要在配置文件配置所有的Tracker节点IP信息就可以啦！ 补充说明：如果Tracker 服务、Storage服务、Nginx服务开机后没有自启动，请执行一下操作并进行重启 12[root@localhost ~]# chkconfig --add fdfs_trackerd[root@localhost ~]# chkconfig --add fdfs_storaged 编辑目录下的/etc/rc.d/rc.local,内容如下：1234567891011121314#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure# that this script will be executed during boot.touch /var/lock/subsys/local/usr/local/nginx/sbin/nginx 主要增加了Nginx的启动，之后进行文件生效，重新启动系统 123[root@localhost ~]# chmod +x /etc/rc.d/rc.local[root@localhost ~]# source /etc/rc.d/rc.local [root@localhost ~]# reboot]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hive高级操作]]></title>
    <url>%2Fblogs%2F2018%2F02%2F01%2FHive%E9%AB%98%E7%BA%A7%E6%9F%A5%E8%AF%A2%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Hive高级操作1.使用LIKE、AS创建表,表重命名，添加、修改、删除列 表结构数据复制根据已存在的表结构，使用like关键字，复制一个表结构一模一样的新表 123456789101112hive&gt; create table student_info2 like student_info;OKTime taken: 0.73 secondshive&gt; show tables;OKemployeestudent_infostudent_info2student_school_infostudent_school_info_external_partitionstudent_school_info_partitionTime taken: 0.15 seconds, Fetched: 6 row(s) 根据已经存在的表，使用as关键字，创建一个与查询结果字段一致的表，同时将查询结果数据插入到新表 1create table student_info3 as select * from student_info; 只有student_id,name两个字段的表 1create table student_info4 as select student_id,name from student_info; 表重命名student_info4表重命名为student_id_name 1alter table student_info4 rename to student_id_name; 添加列给student_info3表添加性别列,新添加的字段会在所有列最后，分区列之前，在添加新列之前已经存在的数据文件中。如果没有新添加列对应的数据，在查询的时候显示为空。添加多个列用逗号隔开。 123hive&gt; alter table student_info3 add columns(gender string comment &apos;性别&apos;);OKTime taken: 0.185 seconds 删除列或修改列修改列，将继续存在的列再定义一遍，需要替换的列重新定义 123hive&gt; alter table student_info3 replace columns(student_id string,name string,age int,origin string,gender2 int);OKTime taken: 0.422 seconds 删除列,将继续存在的列再定义一遍，需要删除的列不再定义 123hive&gt; alter table student_info3 replace columns(student_id string,name string,age int,origin string);OKTime taken: 0.529 seconds 2.分桶表使用 创建分桶表按照指定字段取它的hash散列值分桶,创建学生入学信息分桶表 字段名称 类型 注释 分桶字段 student_id string 学生ID 是 name string 姓名 否 age int 年龄 否 origin string 学院ID 否 1234567891011create table rel.student_info_bucket(student_id string,name string,age int,origin string)clustered by (student_id) sorted by (student_id asc) into 4 buckets row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile; 分桶表插入数据向student_info_bucket分桶表插入数据 123456set hive.enforce.bucketing = true;set mapreduce.job.reduces=4;insert overwrite table student_info_bucket select student_id,name,age,origin from student_info cluster by(student_id); 查看hdfs分桶文件 123456[root@hadoop01 ~]# hadoop fs -ls /user/hive/warehouse/rel.db/student_info_bucketFound 4 items-rwxr-xr-x 3 hadoop supergroup 78 2018-01-24 19:33 /user/hive/warehouse/rel.db/student_info_bucket/000000_0-rwxr-xr-x 3 hadoop supergroup 84 2018-01-24 19:33 /user/hive/warehouse/rel.db/student_info_bucket/000001_0-rwxr-xr-x 3 hadoop supergroup 80 2018-01-24 19:33 /user/hive/warehouse/rel.db/student_info_bucket/000002_0-rwxr-xr-x 3 hadoop supergroup 81 2018-01-24 19:33 /user/hive/warehouse/rel.db/student_info_bucket/000003_0 说明： 分桶表一般不使用load向分桶表中导入数据，因为load导入数据只是将数据复制到表的数据存储目录下，hive并不会在load的时候对数据进行分析然后按照分桶字段分桶，load只会将一个文件全部导入到分桶表中，并没有分桶。一般采用insert从其他表向分桶表插入数据。分桶表在创建表的时候只是定义表的模型，插入的时候需要做如下操作：在每次执行分桶插入的时候在当前执行的session会话中要设置 1hive.enforce.bucketing=true; 声明本次执行的是一次分桶操作。需要指定reduce个数与分桶的数量相同 1set mapreduce.job.reduces=4， 这样才能保证有多少桶就生成多少个文件。如果定义了按照分桶字段排序，需要在从其他表查询数据过程中将数据按照分区字段排序之后插入各个桶中，分桶表并不会将各分桶中的数据排序。排序和分桶的字段相同的时候使用Cluster by(字段),cluster by 默认按照分桶字段在桶内升序排列，如果需要在桶内降序排列，使用distribute by (col) sort by (col desc)组合实现。 3.导出数据 使用insert将student_info表数据导出到本地指定路径 12insert overwrite local directory &apos;/home/hadoop/apps/hive_test_data/export_data&apos; row format delimited fields terminated by &apos;\t&apos; select * from student_info; 123456789101112131415161718192021[root@hadoop01 export_data]# cat 000000_0 1 xiaoming 20 112 xiaobai 21 313 zhangfei 22 444 likui 19 445 zhaoyun 21 136 zhangsan 20 117 lisi 19 118 wangwu 23 319 zhaofei 19 2110 zhangyan 20 2111 lihe 20 2212 caoyang 17 3213 lihao 19 3214 zhaoming 21 5015 zhouhong 18 5116 yangshuo 23 3317 xiaofei 24 1318 liman 23 1319 qianbao 20 1320 sunce 21 41 导出数据到本地的常用方法1[hadoop@hadoop01 export_data]$ hive -e&quot;select * from rel.student_info&quot;&gt; /home/hadoop/student_info_data.txt 123456789101112131415161718192021[hadoop@hadoop01 ~]$ cat student_info_data.txt 1 xiaoming 20 112 xiaobai 21 313 zhangfei 22 444 likui 19 445 zhaoyun 21 136 zhangsan 20 117 lisi 19 118 wangwu 23 319 zhaofei 19 2110 zhangyan 20 2111 lihe 20 2212 caoyang 17 3213 lihao 19 3214 zhaoming 21 5015 zhouhong 18 5116 yangshuo 23 3317 xiaofei 24 1318 liman 23 1319 qianbao 20 1320 sunce 21 41 默认结果分隔符：’\t’ 4.关联查询 创建2张表 1234567891011121314151617create table rel.a(id int,name string)row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile;create table rel.b(id int,name string)row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile; 导入数据 12345678910hive&gt; load data local inpath &apos;/home/hadoop/apps/hive_test_data/a_join_data&apos; into table a;Loading data to table rel.aTable rel.a stats: [numFiles=1, totalSize=61]OKTime taken: 1.79 secondshive&gt; load data local inpath &apos;/home/hadoop/apps/hive_test_data/b_join_data&apos; into table b;Loading data to table rel.bTable rel.b stats: [numFiles=1, totalSize=38]OKTime taken: 0.562 seconds inner或inner join两个表通过id关联，只把id值相等的数据查询出来。join的查询结果与inner join的查询结果相同。 1select * from a join b on a.id=b.id; 等同于 12345678select * from a inner join b on a.id=b.id;.....OK1 a 1 AA2 b 2 BB3 c 3 CC6 f 6 FFTime taken: 44.337 seconds, Fetched: 4 row(s) full outer join或full join 两个表通过id关联，把两个表的数据全部查询出来123456789101112131415161718OK1 a 1 AA2 b 2 BB3 c 3 CC4 d NULL NULL5 e NULL NULL6 f 6 FF7 g NULL NULL8 h NULL NULL9 i NULL NULL10 j NULL NULL11 k NULL NULL12 l NULL NULL13 m NULL NULL14 n NULL NULLNULL NULL 20 TTNULL NULL 21 UUNULL NULL 22 vv left join 左连接时，左表中出现的join字段都保留，右表没有连接上的都为空 123456789101112131415OK1 a 1 AA2 b 2 BB3 c 3 CC4 d NULL NULL5 e NULL NULL6 f 6 FF7 g NULL NULL8 h NULL NULL9 i NULL NULL10 j NULL NULL11 k NULL NULL12 l NULL NULL13 m NULL NULL14 n NULL NULL right join 右连接时，右表中出现的join字段都保留，左表没有连接上的都是空 1select * from a right join b on a.id=b.id; 123456789OK1 a 1 AA2 b 2 BB3 c 3 CC6 f 6 FFNULL NULL 20 TTNULL NULL 21 UUNULL NULL 22 vvTime taken: 25.188 seconds, Fetched: 7 row(s) left semi join 左半连接实现了类似IN/EXISTS的查询语义，输出符合条件的左表内容。hive不支持in …exists这种关系型数据库中的子查询结构，hive暂时不支持右半连接。例如： 1select a.id, a.name from a where a.id in (select b.id from b); 使用Hive对应于如下语句： 1select a.id,a.name from a left semi join b on a.id = b.id; 123456OK1 a2 b3 c6 fTime taken: 27.42 seconds, Fetched: 4 row(s) map side join 使用分布式缓存将小表数据加载都各个map任务中，在map端完成join，map任务输出后，不需要将数据拷贝到reducer阶段再进行join，降低的数据在网络节点之间传输的开销。多表关联数据倾斜优化的一种手段。多表连接，如果只有一个表比较大，其他表都很小，则join操作会转换成一个只包含map的Job。运行日志中会出现Number of reduce tasks is set to 0 since there’s no reduce operator没有reduce的提示。例如： 1select /*+ mapjoin(b) */ a.id, a.name from a join b on a.id = b.id 1234567Total MapReduce CPU Time Spent: 1 seconds 320 msecOK1 a2 b3 c6 fTime taken: 25.538 seconds, Fetched: 4 row(s) 5.Hive内置函数创建用户评分表 123456789create table rel.user_core_info(user_id string,age int,gender string,core int)row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos;stored as textfile; 导入数据 1load data local inpath &apos;/home/hadoop/apps/hive_test_data/user_core.txt&apos; into table rel.user_core_info; 条件函数 case when 语法1：CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END 说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f例如： 1234hive&gt; select case 1 when 2 then &apos;two&apos; when 1 then &apos;one&apos; else &apos;zero&apos; end;OKoneTime taken: 0.152 seconds, Fetched: 1 row(s) 语法2：CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END说明：如果a为TRUE，则返回b；如果c为TRUE，则返回d；否则返回e例如： 1234hive&gt; select case when 1=2 then &apos;two&apos; when 1=1 then &apos;one&apos; else &apos;zero&apos; end;OKoneTime taken: 0.33 seconds, Fetched: 1 row(s) 查询用户评分表，每个年龄段的最大评分值 123456select gender,case when age&lt;=20 then &apos;p0&apos; when age&gt;20 and age&lt;=50 then &apos;p1&apos; when age&gt;=50 then &apos;p3&apos; else &apos;p0&apos; end,max(core) max_corefrom rel.user_core_info group by gender,case when age&lt;=20 then &apos;p0&apos; when age&gt;20 and age&lt;=50 then &apos;p1&apos; when age&gt;=50 then &apos;p3&apos; else &apos;p0&apos; end; 结果为： 12345678OKfemale p0 90female p1 95female p3 90male p0 80male p1 80male p3 80Time taken: 28.461 seconds, Fetched: 6 row(s) 自定义UDF函数 当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。UDF 作用于单个数据行，产生一个数据行作为输出。步骤： 先开发一个java类，继承UDF，并重载evaluate方法 打成jar包上传到服务器 在使用的时候将jar包添加到hive的classpath 1hive&gt;add jar /home/hadoop/apps/hive_test_data/HiveUdfPro-1.0-SNAPSHOT.jar; 创建临时函数与开发好的java class关联 1hive&gt;create temporary function age_partition as &apos;cn.chinahadoop.udf.AgePartitionFunction&apos;; 即可在hql中使用自定义的函数新建Maven 项目Pom 信息如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.yongliang.udf&lt;/groupId&gt; &lt;artifactId&gt;HiveUdfPro&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;HiveUdfPro&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default-compile&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 新建类继承UDF 123456789101112131415161718192021package com.yongliang.udf;import org.apache.hadoop.hive.ql.exec.UDF;/** * 创建时间 : 2018/1/27 15:35 * 类描述 : Hive UDF自定义函数，作用于单个数据行，产生一个数据行作为输出 * @author zhangyonglaing */public class AgePartitionFunction extends UDF &#123; public String evaluate(int age) &#123; String partition = &quot;p0&quot;; if(age &lt;=20)&#123; partition = &quot;p0&quot;; &#125;else if(age &gt; 20 &amp;&amp; age &lt;=50)&#123; partition = &quot;p1&quot;; &#125;else if(age &gt; 50)&#123; partition = &quot;p2&quot;; &#125; return partition; &#125;&#125; 将项目进行打包 Hive UDF 自定义函数 说明: 如出现以下异常信息：Failed to execute goal on project hive-exec:Could not resolve dependencies for project org.apache.hive:hive-exec:jar:2.3.0:Could not find artifact org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde in alimaven (http://maven.aliyun.com/nexus/content/groups/public/) -&gt; [Help 1] 错误异常信息 请手动下载Jar包pentaho-aggdesigner-algorithm/5.1.5-jhyde.jar 下载地址：https://public.nexus.pentaho.org/content/groups/omni/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/ 将Jar包放置在本地Maven仓库org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde路径下，之后进行重新打包。 将jar包添加到hive的classpath 123hive&gt; add jar /home/hadoop/apps/HiveUdfPro-1.0-SNAPSHOT.jar;Added [/home/hadoop/apps/HiveUdfPro-1.0-SNAPSHOT.jar] to class pathAdded resources: [/home/hadoop/apps/HiveUdfPro-1.0-SNAPSHOT.jar] 创建临时函数与开发好的java class关联 1hive&gt; create temporary function age_partition as &apos;com.yongliang.udf.AgePartitionFunction&apos;; 在hql中使用自定义的函数 123456select gender,age_partition(age),max(core) max_corefrom rel.user_core_info group by gender,age_partition(age); 结果为：1234567OKfemale p0 90female p1 95female p2 90male p0 80male p1 80male p2 80]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblogs%2F2018%2F01%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
